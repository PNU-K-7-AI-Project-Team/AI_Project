{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e686ad-ffce-43f1-b82a-0e0714496880",
   "metadata": {},
   "source": [
    "# **라벨링이 있는 UCI-HAR 데이터셋을 활용하여 행동 라벨을 추가하는 방법**\n",
    "***\n",
    "> **UCI-HAR 데이터셋을 사용하여 LSTM 모델을 학습하고, 이를 라벨이 없는 데이터에 적용하여 활동 라벨을 예측**   \n",
    "> 전체 프로세스:\n",
    "> 1. UCI-HAR 데이터 로드 및 전처리 (PyTorch Tensor로 변환)\n",
    "> 2. LSTM 모델 정의\n",
    "> 3. 모델 학습\n",
    "> 4. 라벨 없는 데이터에 대해 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f3ce42-3cb4-4dec-aafc-b829057f218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edff10f-24d0-4807-b5fe-c4fde63799e8",
   "metadata": {},
   "source": [
    "## **1. UCI-HAR 데이터 로드 및 전처리**\n",
    "***\n",
    "> UCI-HAR 데이터셋을 로드   \n",
    "> 자이로스코프 데이터를 가져옴   \n",
    "> 데이터를 정규화하고 시계열 형태로 변환하여 LSTM 모델 학습을 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eeae2778-4ddb-401e-9f7d-9b1ff46158f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## **1. UCI-HAR 데이터 로드 및 전처리**\n",
    "# GPU 또는 CPU 설정 통일\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# UCI-HAR 데이터셋 경로\n",
    "uci_har_path = './원본 데이터/UCI HAR Dataset/'\n",
    "\n",
    "# 자이로스코프 데이터 로드 함수\n",
    "def load_ucihar_data():\n",
    "    # 파일 경로 설정\n",
    "    gyro_x_train_path = uci_har_path + 'train/Inertial Signals/body_gyro_x_train.txt'\n",
    "    gyro_y_train_path = uci_har_path + 'train/Inertial Signals/body_gyro_y_train.txt'\n",
    "    gyro_z_train_path = uci_har_path + 'train/Inertial Signals/body_gyro_z_train.txt'\n",
    "    labels_train_path = uci_har_path + 'train/y_train.txt'\n",
    "    \n",
    "    # 자이로스코프 데이터 로드\n",
    "    gyro_x_train = pd.read_csv(gyro_x_train_path, sep='\\s+', header=None).values\n",
    "    gyro_y_train = pd.read_csv(gyro_y_train_path, sep='\\s+', header=None).values\n",
    "    gyro_z_train = pd.read_csv(gyro_z_train_path, sep='\\s+', header=None).values\n",
    "    \n",
    "    # 라벨 데이터 로드\n",
    "    labels_train = pd.read_csv(labels_train_path, sep='\\s+', header=None).values - 1  # 0부터 시작하도록 -1 처리\n",
    "    \n",
    "    # 자이로 데이터를 결합하여 (samples, timesteps, features) 형태로 만듦\n",
    "    X_train = np.stack([gyro_x_train, gyro_y_train, gyro_z_train], axis=-1)\n",
    "    \n",
    "    return X_train, labels_train\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "X_train, y_train = load_ucihar_data()\n",
    "\n",
    "# 데이터 정규화 (PyTorch 텐서로 변환 전에 적용)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train.reshape(-1, X_train.shape[-1])).reshape(X_train.shape)\n",
    "\n",
    "# PyTorch 텐서로 변환\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# 데이터 증강: 가우시안 잡음 추가 함수\n",
    "def add_gaussian_noise(data, noise_factor=0.05):\n",
    "    noise = np.random.randn(*data.shape)\n",
    "    augmented_data = data + noise_factor * noise\n",
    "    augmented_data = np.clip(augmented_data, -1.0, 1.0)  # 데이터 범위를 유지\n",
    "    return augmented_data\n",
    "\n",
    "# 증강된 데이터 생성\n",
    "X_train_augmented = add_gaussian_noise(X_train)\n",
    "\n",
    "# 증강된 데이터를 추가로 텐서 변환\n",
    "X_train_tensor_augmented = torch.tensor(X_train_augmented, dtype=torch.float32)\n",
    "\n",
    "# 증강된 데이터와 원본 데이터 결합\n",
    "X_train_tensor_full = torch.cat([X_train_tensor, X_train_tensor_augmented], dim=0)\n",
    "y_train_tensor_full = torch.cat([y_train_tensor, y_train_tensor], dim=0)\n",
    "\n",
    "# 데이터셋을 학습 및 검증 데이터로 나누기 (train/val split)\n",
    "X_train_tensor, X_val_tensor, y_train_tensor, y_val_tensor = train_test_split(X_train_tensor_full, \n",
    "                                                                              y_train_tensor_full, \n",
    "                                                                              test_size=0.2, \n",
    "                                                                              random_state=42)\n",
    "\n",
    "# PyTorch 데이터셋 클래스 정의\n",
    "class UCIHARData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_dataset = UCIHARData(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = UCIHARData(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa32b7df-96c1-4b2f-918f-0dbe3fe53428",
   "metadata": {},
   "source": [
    "## **2. LSTM 모델 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da6e05c9-3c1c-4e37-8f97-0432240f1c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM 정의\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        \n",
    "        # Batch Normalization 추가\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "\n",
    "        # 드롭아웃 추가\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Fully connected layer 정의\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # LSTM의 출력을 받아 마지막 타임스텝에서 예측값 도출\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.batch_norm(out[:, -1, :])  # Batch Normalization 적용\n",
    "        out = self.dropout(out)  # 드롭아웃 적용\n",
    "        out = self.fc(out)  # 마지막 타임스텝의 출력을 사용하여 분류\n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "# 모델 인스턴스 생성\n",
    "input_size = X_train.shape[2]  # 자이로스코프의 X, Y, Z 축 (feature 수)\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "num_classes = 6  # 6개의 활동 클래스 (walking, sitting 등)\n",
    "dropout_prob = 0.6\n",
    "\n",
    "model = LSTMModel(input_size=input_size, \n",
    "                  hidden_size=hidden_size, \n",
    "                  num_layers=num_layers, \n",
    "                  num_classes=num_classes, \n",
    "                  dropout_prob=dropout_prob)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류 문제에 적합한 손실 함수\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adam 옵티마이저"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff55b9e-3506-4ace-b7f4-94fa22885ecb",
   "metadata": {},
   "source": [
    "## **3. 모델 학습**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e90b58-5d9b-4045-a33e-f898389c5f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEE\\anaconda3\\envs\\dh\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 1.3608, Training Accuracy: 33.13%\n",
      "Validation Accuracy: 39.51%\n",
      "Best model saved with accuracy: 39.51%\n",
      "Epoch [2/20], Training Loss: 1.2408, Training Accuracy: 40.12%\n",
      "Validation Accuracy: 30.23%\n",
      "Epoch [3/20], Training Loss: 1.4483, Training Accuracy: 31.22%\n",
      "Validation Accuracy: 38.97%\n",
      "Epoch [4/20], Training Loss: 1.0188, Training Accuracy: 46.41%\n",
      "Validation Accuracy: 52.26%\n",
      "Best model saved with accuracy: 52.26%\n",
      "Epoch [5/20], Training Loss: 0.8505, Training Accuracy: 55.85%\n",
      "Validation Accuracy: 55.93%\n",
      "Best model saved with accuracy: 55.93%\n",
      "Epoch [6/20], Training Loss: 0.7937, Training Accuracy: 57.60%\n",
      "Validation Accuracy: 60.32%\n",
      "Best model saved with accuracy: 60.32%\n",
      "Epoch [7/20], Training Loss: 0.7296, Training Accuracy: 59.92%\n",
      "Validation Accuracy: 61.85%\n",
      "Best model saved with accuracy: 61.85%\n",
      "Epoch [8/20], Training Loss: 0.7047, Training Accuracy: 60.62%\n",
      "Validation Accuracy: 62.73%\n",
      "Best model saved with accuracy: 62.73%\n",
      "Epoch [9/20], Training Loss: 0.6799, Training Accuracy: 61.51%\n",
      "Validation Accuracy: 63.04%\n",
      "Best model saved with accuracy: 63.04%\n",
      "Epoch [10/20], Training Loss: 0.6703, Training Accuracy: 61.58%\n",
      "Validation Accuracy: 61.00%\n",
      "Epoch [11/20], Training Loss: 0.6696, Training Accuracy: 61.68%\n",
      "Validation Accuracy: 60.93%\n",
      "Epoch [12/20], Training Loss: 0.6782, Training Accuracy: 62.07%\n",
      "Validation Accuracy: 61.75%\n",
      "Epoch [13/20], Training Loss: 0.6500, Training Accuracy: 62.72%\n",
      "Validation Accuracy: 63.21%\n",
      "Best model saved with accuracy: 63.21%\n",
      "Epoch [14/20], Training Loss: 0.6793, Training Accuracy: 62.19%\n",
      "Validation Accuracy: 61.54%\n",
      "Epoch [15/20], Training Loss: 0.6594, Training Accuracy: 63.29%\n",
      "Validation Accuracy: 64.81%\n",
      "Best model saved with accuracy: 64.81%\n",
      "Epoch [16/20], Training Loss: 0.6383, Training Accuracy: 64.11%\n",
      "Validation Accuracy: 64.50%\n",
      "Epoch [17/20], Training Loss: 0.6553, Training Accuracy: 64.32%\n",
      "Validation Accuracy: 65.56%\n",
      "Best model saved with accuracy: 65.56%\n",
      "Epoch [18/20], Training Loss: 0.6386, Training Accuracy: 64.20%\n",
      "Validation Accuracy: 59.03%\n",
      "Epoch [19/20], Training Loss: 0.6935, Training Accuracy: 64.06%\n",
      "Validation Accuracy: 65.56%\n",
      "Epoch [20/20], Training Loss: 0.6112, Training Accuracy: 66.12%\n",
      "Validation Accuracy: 67.09%\n",
      "Best model saved with accuracy: 67.09%\n"
     ]
    }
   ],
   "source": [
    "# 체크포인트 저장 함수\n",
    "def save_checkpoint(epoch, model, optimizer, filename=\"checkpoint.pth.tar\"):\n",
    "    state = {'epoch': epoch,\n",
    "             'state_dict': model.state_dict(),\n",
    "             'optimizer': optimizer.state_dict()}\n",
    "    torch.save(state, filename)\n",
    "\n",
    "# 최고 검증 정확도를 저장하기 위한 변수\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "# 스케줄러 정의 (ReduceLROnPlateau)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                       mode='min', \n",
    "                                                       factor=0.5, \n",
    "                                                       patience=2, \n",
    "                                                       verbose=True)\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, patience=3):\n",
    "    global best_val_accuracy\n",
    "    model.train()\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # 훈련 루프\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).squeeze()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.cpu() == labels.cpu()).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss/len(train_loader):.4f}, Training Accuracy: {accuracy:.2f}%\")\n",
    "  \n",
    "        # 검증 데이터셋에서 정확도 및 손실 계산\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).squeeze()\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels) # 검증 손실 계산\n",
    "                val_loss += loss.item() # 검증 손실 누적\n",
    "\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted.cpu() == labels.cpu()).sum().item()\n",
    "\n",
    "        val_accuracy = (val_correct / val_total) * 100\n",
    "        val_loss = val_loss / len(val_loader)  # 평균 검증 손실 계산\n",
    "\n",
    "        print(f\"Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # **스케줄러 호출**\n",
    "        scheduler.step(val_loss)  # 검증 손실에 따라 학습률 조정\n",
    "\n",
    "        # **최고 검증 정확도 갱신 및 모델 저장**\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            save_checkpoint(epoch, model, optimizer, filename=\"best_model.pth.tar\")\n",
    "            print(f\"Best model saved with accuracy: {best_val_accuracy:.2f}%\")\n",
    "            early_stop_counter = 0\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        # 조기 종료: patience 만큼 성능 개선이 없으면 학습 중단\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        model.train()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 모델 학습\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20, patience=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eacc74-c8b7-4243-bf26-99124fc5d4e1",
   "metadata": {},
   "source": [
    "## **4. 라벨 없는 데이터에 대해 예측**\n",
    "***\n",
    "> 학습된 모델을 사용하여 라벨이 없는 자이로스코프 데이터에 대해 행동을 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8aaecd53-a4ac-4d30-8bab-18bf8c08497f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9094440960 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     24\u001b[0m     X_unlabeled \u001b[38;5;241m=\u001b[39m X_unlabeled\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 25\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     predicted_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# 예측된 라벨을 활동 이름으로 매핑\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dh\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dh\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m, in \u001b[0;36mLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# LSTM의 출력을 받아 마지막 타임스텝에서 예측값 도출\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m     out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])  \u001b[38;5;66;03m# Batch Normalization 적용\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(out)  \u001b[38;5;66;03m# 드롭아웃 적용\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dh\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dh\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dh\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:891\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    889\u001b[0m unsorted_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hx \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 891\u001b[0m     h_zeros \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_directions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_hidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    894\u001b[0m     c_zeros \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers \u001b[38;5;241m*\u001b[39m num_directions,\n\u001b[0;32m    895\u001b[0m                           max_batch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[0;32m    896\u001b[0m                           dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    897\u001b[0m     hx \u001b[38;5;241m=\u001b[39m (h_zeros, c_zeros)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 9094440960 bytes."
     ]
    }
   ],
   "source": [
    "# 라벨 없는 데이터 로드 및 전처리 (UCI-HAR와 동일한 형식으로 처리)\n",
    "gyro_data = pd.read_csv('./원본 데이터/자이로 데이터.csv')\n",
    "gyro_data_values = gyro_data[['X', 'Y', 'Z']].values\n",
    "gyro_data_normalized = scaler.transform(gyro_data_values)\n",
    "\n",
    "# 슬라이딩 윈도우 함수 정의\n",
    "def create_sliding_windows(data, seq_len):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_len + 1):\n",
    "        sequences.append(data[i:i + seq_len])\n",
    "    return np.array(sequences)\n",
    "\n",
    "# 라벨 없는 데이터에 대해 시퀀스 길이 맞추기\n",
    "seq_len = X_train.shape[1]  # 학습에 사용된 시퀀스 길이\n",
    "gyro_data_sliding_windows = create_sliding_windows(gyro_data_normalized, seq_len)\n",
    "\n",
    "# PyTorch 텐서로 변환\n",
    "X_unlabeled = torch.tensor(gyro_data_sliding_windows, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# 모델 예측\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    X_unlabeled = X_unlabeled.to(device)\n",
    "    outputs = model(X_unlabeled)\n",
    "    predicted_labels = torch.argmax(outputs, dim=1)\n",
    "\n",
    "# 예측된 라벨을 활동 이름으로 매핑\n",
    "activity_labels = {0: \"walking\", 1: \"walking_upstairs\", 2: \"walking_downstairs\", 3: \"sitting\", 4: \"standing\", 5: \"laying\"}\n",
    "predicted_activities = [activity_labels[label.item()] for label in predicted_labels]\n",
    "\n",
    "# 결과 출력\n",
    "for i, activity in enumerate(predicted_activities[:10]):  # 예시로 10개의 결과만 출력\n",
    "    print(f\"Sample {i}: {activity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
