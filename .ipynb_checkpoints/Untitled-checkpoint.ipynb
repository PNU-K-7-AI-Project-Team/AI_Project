{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986b6683-8d4b-452c-999b-7d3600486bcb",
   "metadata": {},
   "source": [
    "> UCI-HAR와 자이로 데이터 간의 차이를 줄이고, 자이로 데이터에 적합한 행동 라벨링을 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada6549-69b7-49a6-b716-143ccd015520",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec6ef79-f095-4874-a7b3-abbde581675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 라이브러리 및 데이터셋 로드\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from scipy.signal import butter, filtfilt, resample\n",
    "\n",
    "# GPU 사용 여부 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb51601-77a7-425f-86dd-2afdc0e62774",
   "metadata": {},
   "source": [
    "## 2. UCI-HAR 데이터 로드 및 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b692459-a690-4403-85cb-ac5ca704d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 자이로 데이터 전처리 추가 (UCI-HAR에 맞추기)\n",
    "\n",
    "# 저역통과 필터 적용 함수 (5Hz cutoff)\n",
    "def butter_lowpass(cutoff, fs, order=4):\n",
    "    nyq = 0.5 * fs  # Nyquist frequency\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def apply_lowpass_filter(data, cutoff=5, fs=50):\n",
    "    b, a = butter_lowpass(cutoff, fs)\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "# 자이로 데이터 로드 및 전처리 (UCI-HAR에 맞추기)\n",
    "def preprocess_gyro_data(gyro_path, current_freq=100, desired_freq=50, window_size=128, stride=64):\n",
    "    gyro_data = pd.read_csv(gyro_path)\n",
    "    \n",
    "    # 샘플링 속도 맞추기 (다운샘플링)\n",
    "    n_samples = int(len(gyro_data) * desired_freq / current_freq)\n",
    "    gyro_data_resampled = resample(gyro_data[['X', 'Y', 'Z']].values, n_samples)\n",
    "    \n",
    "    # 저역통과 필터 적용\n",
    "    gyro_data_filtered = apply_lowpass_filter(gyro_data_resampled, cutoff=5, fs=desired_freq)\n",
    "    \n",
    "    # 축별 정규화\n",
    "    scaler = StandardScaler()\n",
    "    gyro_data_normalized = scaler.fit_transform(gyro_data_filtered)\n",
    "    \n",
    "    # 슬라이딩 윈도우 적용 (128 샘플, 50% 겹침)\n",
    "    def create_sliding_windows(data, window_size=128, stride=64):\n",
    "        num_windows = (len(data) - window_size) // stride + 1\n",
    "        windows = np.array([data[i:i+window_size] for i in range(0, len(data) - window_size + 1, stride)])\n",
    "        return windows\n",
    "    \n",
    "    gyro_data_windows = create_sliding_windows(gyro_data_normalized, window_size, stride)\n",
    "    \n",
    "    return torch.tensor(gyro_data_windows, dtype=torch.float32)\n",
    "\n",
    "# 자이로 데이터 전처리 실행\n",
    "gyro_data_tensor = preprocess_gyro_data('./원본 데이터/자이로 데이터.csv')\n",
    "\n",
    "# 3. UCI-HAR 데이터 로드 및 전처리 (기존 코드 유지)\n",
    "\n",
    "uci_har_path = './원본 데이터/UCI HAR Dataset/'\n",
    "\n",
    "def load_ucihar_data():\n",
    "    gyro_x_train_path = uci_har_path + 'train/Inertial Signals/body_gyro_x_train.txt'\n",
    "    gyro_y_train_path = uci_har_path + 'train/Inertial Signals/body_gyro_y_train.txt'\n",
    "    gyro_z_train_path = uci_har_path + 'train/Inertial Signals/body_gyro_z_train.txt'\n",
    "    labels_train_path = uci_har_path + 'train/y_train.txt'\n",
    "    \n",
    "    gyro_x_train = pd.read_csv(gyro_x_train_path, sep='\\s+', header=None).values\n",
    "    gyro_y_train = pd.read_csv(gyro_y_train_path, sep='\\s+', header=None).values\n",
    "    gyro_z_train = pd.read_csv(gyro_z_train_path, sep='\\s+', header=None).values\n",
    "    labels_train = pd.read_csv(labels_train_path, sep='\\s+', header=None).values - 1\n",
    "    \n",
    "    X_train = np.stack([gyro_x_train, gyro_y_train, gyro_z_train], axis=-1)\n",
    "    return X_train, labels_train\n",
    "\n",
    "X_train, y_train = load_ucihar_data()\n",
    "\n",
    "# 축별 정규화\n",
    "scaler = StandardScaler()\n",
    "for i in range(X_train.shape[2]):\n",
    "    X_train[:, :, i] = scaler.fit_transform(X_train[:, :, i])\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4a63ed-cfc3-4416-bfdd-1d2ebd0885d1",
   "metadata": {},
   "source": [
    "## 4. 데이터 증강 및 불균형 해결(SMOTE 적용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79977938-bcef-4f15-8457-cc194d388ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. 데이터 증강 및 불균형 해결\n",
    "\n",
    "def add_gaussian_noise(data, noise_factor=0.05):\n",
    "    noise = np.random.randn(*data.shape)\n",
    "    augmented_data = data + noise_factor * noise\n",
    "    augmented_data = np.clip(augmented_data, -1.0, 1.0)\n",
    "    return augmented_data\n",
    "\n",
    "X_train_augmented = add_gaussian_noise(X_train)\n",
    "\n",
    "# 증강된 데이터 결합\n",
    "X_train_full = np.concatenate([X_train, X_train_augmented], axis=0)\n",
    "y_train_full = np.concatenate([y_train, y_train], axis=0)\n",
    "\n",
    "# 평면화 후 RandomOverSampler 사용\n",
    "X_train_flattened = X_train_full.reshape(X_train_full.shape[0], -1)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_train_res, y_train_res = ros.fit_resample(X_train_flattened, y_train_full)\n",
    "\n",
    "# 시계열 형태로 복원\n",
    "X_train_res = X_train_res.reshape(-1, X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# 텐서로 변환\n",
    "X_train_res_tensor = torch.tensor(X_train_res, dtype=torch.float32)\n",
    "y_train_res_tensor = torch.tensor(y_train_res, dtype=torch.long)\n",
    "\n",
    "# 학습/검증 데이터 분리\n",
    "X_train_tensor, X_val_tensor, y_train_tensor, y_val_tensor = train_test_split(X_train_res_tensor, \n",
    "                                                                              y_train_res_tensor, \n",
    "                                                                              test_size=0.2,\n",
    "                                                                              random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f66a65-bfa7-44a0-ace2-918bdc495d51",
   "metadata": {},
   "source": [
    "## 4. 데이터셋 로더 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c18092f1-b7c1-48a4-8e10-a8a817b40926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 데이터셋 로더 생성\n",
    "class UCIHARData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = UCIHARData(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = UCIHARData(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba567f6-5e53-4da3-bf91-2be92ef4f88c",
   "metadata": {},
   "source": [
    "## 5. LSTM, GRU, CNN-LSTM, BiLSTM, Transformer 모델 정의 및 성능 개선(앙상블)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98fa2695-7fae-44c3-8b44-089e15fde875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 정의\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.batch_norm(out[:, -1, :])\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# GRU 모델 정의\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = self.batch_norm(out[:, -1, :])\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# CNN-LSTM 모델 정의\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.lstm = nn.LSTM(64, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# BiLSTM 모델 정의\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob, bidirectional=True)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.bilstm(x)\n",
    "        out = self.batch_norm(out[:, -1, :])\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Transformer 모델 정의\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, num_heads, num_layers, num_classes, dropout_prob):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_embedding = nn.Linear(input_size, 128)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 200, 128))  # 시퀀스 길이가 100이라고 가정\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=num_heads, dropout=dropout_prob)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size) -> (seq_len, batch_size, input_size)\n",
    "        x = self.input_embedding(x)\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.transformer(x.permute(1, 0, 2))  # (seq_len, batch_size, d_model)\n",
    "        out = self.fc(x[-1, :, :])  # 마지막 타임스텝의 출력만 사용\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c7682c-24fa-402d-ab3a-6b4b4bd2cb76",
   "metadata": {},
   "source": [
    "## 6. 앙상블 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16ab5040-8752-4085-8c10-735abab8b5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LEE\\anaconda3\\envs\\dh\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# 앙상블에 포함될 모델 리스트 정의\n",
    "models = [\n",
    "    LSTMModel(input_size=X_train.shape[2], hidden_size=128, num_layers=3, num_classes=6, dropout_prob=0.6).to(device),\n",
    "    GRUModel(input_size=X_train.shape[2], hidden_size=128, num_layers=3, num_classes=6, dropout_prob=0.6).to(device),\n",
    "    CNNLSTMModel(input_size=X_train.shape[2], hidden_size=128, num_layers=3, num_classes=6, dropout_prob=0.6).to(device),\n",
    "    BiLSTMModel(input_size=X_train.shape[2], hidden_size=128, num_layers=3, num_classes=6, dropout_prob=0.6).to(device),\n",
    "    TransformerModel(input_size=X_train.shape[2], num_heads=4, num_layers=2, num_classes=6, dropout_prob=0.6).to(device)\n",
    "]\n",
    "\n",
    "# 모델들의 예측값을 앙상블로 처리\n",
    "# 가중 평균을 적용한 앙상블 예측\n",
    "def ensemble_predict(models, X_unlabeled, weights=[1, 1, 1, 1, 1]):\n",
    "    predictions = []\n",
    "    for i, model in enumerate(models):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_unlabeled)\n",
    "            predictions.append(weights[i] * torch.argmax(outputs, dim=1))\n",
    "    \n",
    "    final_predictions = torch.mode(torch.stack(predictions), dim=0)[0]\n",
    "    return final_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8c17d8-8100-4c69-8a9e-9d0e9c4a11b0",
   "metadata": {},
   "source": [
    "## 7. 손실 함수, 옵티마이저, 스케줄러 및 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ee838-0a4f-4a46-8a69-f5973facad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 각 모델별로 옵티마이저와 스케줄러 정의\n",
    "optimizers = [\n",
    "    torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4) for model in models\n",
    "]\n",
    "\n",
    "schedulers = [\n",
    "    torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0.0001) for optimizer in optimizers\n",
    "]\n",
    "\n",
    "# 모델 저장 함수\n",
    "def save_checkpoint(epoch, model, optimizer, filename=\"checkpoint.pth.tar\"):\n",
    "    state = {'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "    torch.save(state, filename)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "# 모델 학습 함수\n",
    "def train_model(models, train_loader, val_loader, criterion, optimizers, schedulers, num_epochs=30, patience=5):\n",
    "    global best_val_accuracy\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for model in models:\n",
    "            model.train()\n",
    "        \n",
    "        running_loss = [0.0] * len(models)\n",
    "        correct = [0] * len(models)\n",
    "        total = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device).squeeze()\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # 각 모델별 학습\n",
    "            for j, model in enumerate(models):\n",
    "                optimizers[j].zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizers[j].step()\n",
    "                \n",
    "                running_loss[j] += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct[j] += (predicted.cpu() == labels.cpu()).sum().item()\n",
    "\n",
    "        # Training accuracy and loss\n",
    "        for j, model in enumerate(models):\n",
    "            accuracy = 100 * correct[j] / total\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Model {j+1} Training Loss: {running_loss[j]/len(train_loader):.4f}, Training Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Validation loop\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "        \n",
    "        val_loss = [0.0] * len(models)\n",
    "        val_correct = [0] * len(models)\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device).squeeze()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "                for j, model in enumerate(models):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss[j] += loss.item()\n",
    "\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_correct[j] += (predicted.cpu() == labels.cpu()).sum().item()\n",
    "\n",
    "        # Validation accuracy and loss\n",
    "        for j, model in enumerate(models):\n",
    "            val_accuracy = (val_correct[j] / val_total) * 100\n",
    "            val_loss[j] = val_loss[j] / len(val_loader)\n",
    "\n",
    "            print(f\"Model {j+1} Validation Accuracy: {val_accuracy:.2f}%, Validation Loss: {val_loss[j]:.4f}\")\n",
    "\n",
    "            # 스케줄러 갱신\n",
    "            schedulers[j].step()\n",
    "\n",
    "            # Save best model\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                save_checkpoint(epoch, model, optimizers[j], filename=f\"best_model_{j+1}.pth.tar\")\n",
    "                print(f\"Best model {j+1} saved with accuracy: {best_val_accuracy:.2f}%\")\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 모델 학습 실행\n",
    "train_model(models, train_loader, val_loader, criterion, optimizers, schedulers, num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ffcde-3ec3-4776-92b9-cc004933b884",
   "metadata": {},
   "source": [
    "## 8. 슬라이딩 윈도우와 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f6b35-b35d-4ca7-9adf-1b3f540d83ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치 단위로 슬라이딩 윈도우 적용하는 함수\n",
    "def create_sliding_windows_batch(data, seq_len, batch_size):\n",
    "    for start in range(0, len(data) - seq_len + 1, batch_size):\n",
    "        end = min(start + batch_size, len(data) - seq_len + 1)\n",
    "        sequences = [data[i:i + seq_len] for i in range(start, end)]\n",
    "        yield np.array(sequences)\n",
    "\n",
    "# 자이로 데이터 슬라이딩 윈도우 적용 및 예측\n",
    "seq_len = X_train.shape[1]\n",
    "chunk_size = 1000  # 한번에 처리할 데이터 크기\n",
    "\n",
    "all_predictions = []\n",
    "\n",
    "for batch_sliding_windows in create_sliding_windows_batch(gyro_data_tensor.cpu().numpy(), seq_len, chunk_size):\n",
    "    batch_tensor = torch.tensor(batch_sliding_windows, dtype=torch.float32).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        batch_predictions = ensemble_predict(models, batch_tensor)\n",
    "        all_predictions.append(batch_predictions)\n",
    "\n",
    "all_predictions = torch.cat(all_predictions)\n",
    "\n",
    "# 예측된 라벨을 활동 이름으로 매핑\n",
    "activity_labels = {0: \"walking\", 1: \"walking_upstairs\", 2: \"walking_downstairs\", 3: \"sitting\", 4: \"standing\", 5: \"laying\"}\n",
    "predicted_activities = [activity_labels[label.item()] for label in all_predictions]\n",
    "\n",
    "# 예측 결과 출력\n",
    "for i, activity in enumerate(predicted_activities[:10]):  # 예시로 10개의 결과만 출력\n",
    "    print(f\"Sample {i}: {activity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a38691-f8b2-4b16-aeba-9f04c24c1a15",
   "metadata": {},
   "source": [
    "## 9. 성능 평가 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ff3ec-f240-460e-a936-6e29681dd6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과를 CSV 파일로 저장\n",
    "gyro_data = pd.read_csv('./원본 데이터/자이로 데이터.csv')\n",
    "gyro_data['Predicted_Activity'] = predicted_activities\n",
    "gyro_data.to_csv('./원본 데이터/자이로 데이터_예측.csv', index=False)\n",
    "\n",
    "# 예측된 활동별 분포 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=gyro_data['Predicted_Activity'])\n",
    "plt.title('Predicted Activity Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 클러스터링 적용 및 결과 비교\n",
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(gyro_data_tensor.cpu().numpy().reshape(gyro_data_tensor.shape[0], -1))\n",
    "\n",
    "# 클러스터링 결과와 실제 예측 라벨 비교\n",
    "kmeans_accuracy = accuracy_score(all_predictions.cpu().numpy(), kmeans_labels)\n",
    "print(f\"K-Means Clustering Accuracy: {kmeans_accuracy:.2f}\")\n",
    "\n",
    "# Confusion Matrix 및 성능 평가\n",
    "cm = confusion_matrix(all_predictions.cpu().numpy(), kmeans_labels)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(all_predictions.cpu().numpy(), kmeans_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
