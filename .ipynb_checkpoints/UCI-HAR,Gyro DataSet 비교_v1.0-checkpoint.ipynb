{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c01daa3-68bf-46e8-9f85-2d5678754ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 0.0242, Val Loss: 0.0000, Train Acc: 0.9981, Val Acc: 1.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 153\u001b[0m\n\u001b[0;32m    150\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model_with_time.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    152\u001b[0m \u001b[38;5;66;03m# 8. 모델 학습 시작\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# 9. 모델 예측 수행 및 시각화\u001b[39;00m\n\u001b[0;32m    156\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_model_with_time.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[17], line 113\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, epochs)\u001b[0m\n\u001b[0;32m    111\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(X_batch)\n\u001b[0;32m    112\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, torch\u001b[38;5;241m.\u001b[39mzeros(outputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong))  \u001b[38;5;66;03m# dummy labels\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    115\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dh\\lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dh\\lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dh\\lib\\site-packages\\torch\\autograd\\graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    770\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    771\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1. 자이로 데이터 로드 및 전처리 (첫 행 제외)\n",
    "gyro_data = pd.read_csv('./원본 데이터/자이로 데이터.csv', header=0, low_memory=False)\n",
    "\n",
    "# RegisterDate 열 전처리\n",
    "gyro_data['RegisterDate'] = pd.to_datetime(gyro_data['RegisterDate'], errors='coerce')\n",
    "\n",
    "# NaT 값을 가진 행을 제거\n",
    "gyro_data = gyro_data.dropna(subset=['RegisterDate'])\n",
    "\n",
    "# 시간 차이 계산 및 시간 기반 피처 생성\n",
    "gyro_data['time_diff'] = gyro_data['RegisterDate'].diff().dt.total_seconds().fillna(0)\n",
    "gyro_data['hour'] = gyro_data['RegisterDate'].dt.hour\n",
    "gyro_data['minute'] = gyro_data['RegisterDate'].dt.minute\n",
    "\n",
    "# 2. 자이로스코프 3축 데이터 및 시간 기반 피처 결합\n",
    "gyro_features = gyro_data.iloc[:, [2, 3, 4]].values  # X, Y, Z 축\n",
    "time_features = gyro_data[['time_diff', 'hour', 'minute']].values  # 시간 기반 피처\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "gyro_features_scaled = scaler.fit_transform(gyro_features)\n",
    "time_features_scaled = scaler.fit_transform(time_features)\n",
    "\n",
    "# 자이로스코프 데이터와 시간 기반 피처 결합\n",
    "gyro_features_combined = np.hstack((gyro_features_scaled, time_features_scaled))\n",
    "\n",
    "# 3. 시퀀스 생성\n",
    "sequence_length = 128\n",
    "\n",
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    for i in range(0, len(data) - sequence_length + 1, sequence_length):\n",
    "        seq = data[i:i + sequence_length]\n",
    "        if len(seq) == sequence_length:\n",
    "            sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "gyro_sequences = create_sequences(gyro_features_combined, sequence_length)\n",
    "\n",
    "# 텐서로 변환\n",
    "gyro_tensor = torch.tensor(gyro_sequences, dtype=torch.float32)\n",
    "\n",
    "# 데이터셋 분할\n",
    "X_train, X_val = train_test_split(gyro_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "# 4. CNN-LSTM 모델 정의\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout_rate):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=5, padding=2)\n",
    "        self.lstm = nn.LSTM(64, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Conv1d를 위해 차원 변경\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = x.permute(0, 2, 1)  # LSTM을 위해 차원 복구\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.dropout(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# 5. 하이퍼파라미터 설정 및 모델 생성\n",
    "input_size = 6  # 자이로스코프 3축 + 시간 기반 피처 3개 = 6\n",
    "hidden_size = 64\n",
    "output_size = 6  # UCI-HAR 데이터의 행동 클래스 수\n",
    "num_layers = 1\n",
    "dropout_rate = 0.3\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "\n",
    "model = CNNLSTMModel(input_size, hidden_size, output_size, num_layers, dropout_rate)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 6. 데이터 로더 생성\n",
    "train_dataset = TensorDataset(X_train, torch.zeros(len(X_train), dtype=torch.long))  # dummy labels for now\n",
    "val_dataset = TensorDataset(X_val, torch.zeros(len(X_val), dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 7. 모델 학습 및 평가 함수\n",
    "def train_model(model, train_loader, val_loader, epochs):\n",
    "    best_val_loss = float('inf')\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = [], []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for X_batch, _ in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, torch.zeros(outputs.size(0), dtype=torch.long))  # dummy labels\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            correct_train += (predicted == 0).sum().item()  # dummy labels = 0\n",
    "            total_train += predicted.size(0)\n",
    "\n",
    "        train_loss = total_train_loss / len(train_loader)\n",
    "        train_acc = correct_train / total_train\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, _ in val_loader:\n",
    "                val_outputs = model(X_batch)\n",
    "                val_loss = criterion(val_outputs, torch.zeros(val_outputs.size(0), dtype=torch.long))\n",
    "                total_val_loss += val_loss.item()\n",
    "\n",
    "                _, predicted_val = torch.max(val_outputs.data, 1)\n",
    "                correct_val += (predicted_val == 0).sum().item()  # dummy labels = 0\n",
    "                total_val += predicted_val.size(0)\n",
    "\n",
    "        val_loss = total_val_loss / len(val_loader)\n",
    "        val_acc = correct_val / total_val\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "        # Early stopping 적용\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), \"best_model_with_time.pth\")\n",
    "\n",
    "# 8. 모델 학습 시작\n",
    "train_model(model, train_loader, val_loader, num_epochs)\n",
    "\n",
    "# 9. 모델 예측 수행 및 시각화\n",
    "model.load_state_dict(torch.load(\"best_model_with_time.pth\"))\n",
    "model.eval()\n",
    "\n",
    "def predict_in_batches(model, data_tensor, batch_size):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_tensor.size(0), batch_size):\n",
    "            batch_data = data_tensor[i:i + batch_size]\n",
    "            batch_predictions = model(batch_data)\n",
    "            predicted_classes = torch.argmax(batch_predictions, dim=1)\n",
    "            predictions.append(predicted_classes)\n",
    "    return torch.cat(predictions)\n",
    "\n",
    "# 배치 단위 예측 수행\n",
    "predicted_classes = predict_in_batches(model, gyro_tensor, batch_size)\n",
    "\n",
    "# 예측된 라벨을 원본 데이터 크기에 맞춰 확장\n",
    "predicted_labels_expanded = np.repeat(predicted_classes.cpu().numpy(), sequence_length)\n",
    "predicted_labels_expanded = predicted_labels_expanded[:len(gyro_data)]\n",
    "\n",
    "# 10. 원본 데이터에 예측된 라벨 추가 및 시각화\n",
    "gyro_data['predicted_label'] = predicted_labels_expanded\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(range(len(gyro_data)), gyro_data['predicted_label'], s=1, c='orange', label='Predicted Labels')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Predicted Labels')\n",
    "plt.title('Predicted Behavior Labels')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 11. 최종 데이터 저장\n",
    "gyro_data.to_csv('./원본 데이터/자이로 데이터_with_final_labels.csv', index=False)\n",
    "\n",
    "# 12. 정확도 계산\n",
    "accuracy = accuracy_score(gyro_data['cluster_label'], gyro_data['predicted_label'])\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca0b134-c1c0-4ec8-8d3d-35f0e50da8fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
