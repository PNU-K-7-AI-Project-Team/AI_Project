{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61400e5-f292-4fec-9063-ef6baa48a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.fft import fft\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import resample\n",
    "from scipy.stats import skew, kurtosis, entropy, uniform\n",
    "from scipy.signal import find_peaks, stft\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from skorch import NeuralNetClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa626e-4ca4-45fb-99d6-54a4c64d6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 로드 및 전처리\n",
    "\n",
    "# 자이로 데이터 로드 (RegisterDate를 datetime으로 변환)\n",
    "gyro_data = pd.read_csv('./원본 데이터/자이로 데이터.csv')\n",
    "gyro_data['RegisterDate'] = pd.to_datetime(gyro_data['RegisterDate'])\n",
    "\n",
    "# UCI-HAR 데이터 로드\n",
    "uci_har_path = './원본 데이터/UCI HAR Dataset/'\n",
    "gyro_x_train = pd.read_csv(uci_har_path + 'train/Inertial Signals/body_gyro_x_train.txt', sep='\\s+', header=None).values\n",
    "gyro_y_train = pd.read_csv(uci_har_path + 'train/Inertial Signals/body_gyro_y_train.txt', sep='\\s+', header=None).values\n",
    "gyro_z_train = pd.read_csv(uci_har_path + 'train/Inertial Signals/body_gyro_z_train.txt', sep='\\s+', header=None).values\n",
    "\n",
    "# X, Y, Z 축 데이터를 DataFrame으로 병합\n",
    "uci_har_gyro_df = pd.DataFrame({\n",
    "    'X': np.mean(gyro_x_train, axis=1),\n",
    "    'Y': np.mean(gyro_y_train, axis=1),\n",
    "    'Z': np.mean(gyro_z_train, axis=1)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e952d5-7270-41cc-855a-6840e0a7b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 주파수 업샘플링 (UCI-HAR 데이터를 50Hz에서 100Hz로 업샘플링)\n",
    "current_freq = 50  # UCI-HAR 데이터 주파수\n",
    "desired_freq = 100  # 자이로 데이터 주파수\n",
    "\n",
    "# 보간법을 사용한 업샘플링\n",
    "t_current = np.linspace(0, len(uci_har_gyro_df) / current_freq, num=len(uci_har_gyro_df))\n",
    "t_new = np.linspace(0, len(uci_har_gyro_df) / current_freq, num=len(uci_har_gyro_df) * 2)\n",
    "\n",
    "# 새로운 DataFrame에 업샘플링된 데이터 저장\n",
    "uci_har_gyro_df_upsampled = pd.DataFrame({\n",
    "    'X': np.interp(t_new, t_current, uci_har_gyro_df['X']),\n",
    "    'Y': np.interp(t_new, t_current, uci_har_gyro_df['Y']),\n",
    "    'Z': np.interp(t_new, t_current, uci_har_gyro_df['Z'])\n",
    "})\n",
    "\n",
    "# 자이로 데이터를 UCI-HAR 데이터의 길이로 슬라이싱\n",
    "n_samples_uci = len(uci_har_gyro_df)\n",
    "gyro_sliced = gyro_data.iloc[:n_samples_uci].copy()  # UCI-HAR 데이터 길이만큼 자이로 데이터를 슬라이싱\n",
    "\n",
    "# 데이터 정규화 (MinMaxScaler로 바로 inplace로 변환)\n",
    "scaler = MinMaxScaler()\n",
    "gyro_sliced[['X', 'Y', 'Z']] = scaler.fit_transform(gyro_sliced[['X', 'Y', 'Z']])\n",
    "uci_har_gyro_df[['X', 'Y', 'Z']] = scaler.fit_transform(uci_har_gyro_df[['X', 'Y', 'Z']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f30f7a-8d33-4aad-8a9f-bbcad5eab090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMS 함수 정의\n",
    "def rms(values):\n",
    "    return np.sqrt(np.mean(values**2))\n",
    "\n",
    "# 엔트로피 계산 함수 정의\n",
    "def calc_entropy(values):\n",
    "    # 확률 밀도 함수 계산 후 엔트로피 계산\n",
    "    value_prob = np.histogram(values, bins=30, density=True)[0]  # 확률 밀도 함수\n",
    "    return entropy(value_prob + 1e-6)  # 엔트로피 계산\n",
    "\n",
    "# FFT 특징 계산 함수\n",
    "def fft_features(values, n=10):\n",
    "    fft_vals = np.abs(fft(values))  # FFT 계산 후 절댓값을 취함\n",
    "    return np.mean(fft_vals[:n])  # 주파수 성분의 상위 N개 평균 계산\n",
    "\n",
    "# STFT 특징 계산 함수\n",
    "def stft_features(values, n=10):\n",
    "    _, _, Zxx = stft(values)\n",
    "    Zxx_flat = np.abs(Zxx).flatten()  # STFT 결과를 1차원으로 변환\n",
    "    Zxx_mean = np.mean(Zxx_flat)  # 플랫한 결과의 평균값을 계산\n",
    "    return Zxx_mean\n",
    "\n",
    "# RMS, Skewness, Kurtosis, Entropy 계산 및 피크 탐지 함수\n",
    "def calculate_features(df, axis):\n",
    "    df[f'rms_{axis}'] = rms(df[axis])\n",
    "    df[f'skew_{axis}'] = skew(df[axis])\n",
    "    df[f'kurtosis_{axis}'] = kurtosis(df[axis])\n",
    "    df[f'entropy_{axis}'] = calc_entropy(df[axis])\n",
    "    \n",
    "    # 피크 탐지\n",
    "    peaks, _ = find_peaks(df[axis], height=0)\n",
    "    df[f'peaks_{axis}'] = 0\n",
    "    df.loc[peaks, f'peaks_{axis}'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 각 축에 대해 특성 계산\n",
    "for axis in ['X', 'Y', 'Z']:\n",
    "    uci_har_gyro_df_upsampled = calculate_features(uci_har_gyro_df_upsampled, axis)\n",
    "    gyro_sliced = calculate_features(gyro_sliced, axis)\n",
    "\n",
    "# 푸리에 변환 (FFT) 및 STFT 계산 함수\n",
    "def calculate_fft_stft(df, axis, n=10):\n",
    "    # FFT 계산\n",
    "    df[f'fft_{axis}'] = fft_features(df[axis].values, n=n)\n",
    "    \n",
    "    # STFT 계산\n",
    "    stft_result = stft_features(df[axis].values)\n",
    "    df[f'stft_{axis}'] = stft_result\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 각 축에 대해 FFT 및 STFT 계산\n",
    "for axis in ['X', 'Y', 'Z']:\n",
    "    uci_har_gyro_df_upsampled = calculate_fft_stft(uci_har_gyro_df_upsampled, axis)\n",
    "    gyro_sliced = calculate_fft_stft(gyro_sliced, axis)\n",
    "\n",
    "# 데이터 정규화 (MinMaxScaler)\n",
    "scaler = MinMaxScaler()\n",
    "# 컬럼 이름의 대소문자를 일치시킵니다.\n",
    "columns_to_scale = ['fft_X', 'fft_Y', 'fft_Z', 'stft_X', 'stft_Y', 'stft_Z']\n",
    "uci_har_gyro_df_upsampled[columns_to_scale] = scaler.fit_transform(uci_har_gyro_df_upsampled[columns_to_scale])\n",
    "gyro_sliced[columns_to_scale] = scaler.fit_transform(gyro_sliced[columns_to_scale])\n",
    "\n",
    "# 최종 피처 세트 구축\n",
    "X_train_features = np.column_stack((\n",
    "    uci_har_gyro_df_upsampled[['X', 'Y', 'Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['rms_X', 'rms_Y', 'rms_Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['skew_X', 'skew_Y', 'skew_Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['entropy_X', 'entropy_Y', 'entropy_Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['fft_X', 'fft_Y', 'fft_Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['peaks_X', 'peaks_Y', 'peaks_Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['stft_X', 'stft_Y', 'stft_Z']].values\n",
    "))\n",
    "\n",
    "X_gyro_features = np.column_stack((\n",
    "    gyro_sliced[['X', 'Y', 'Z']].values,\n",
    "    gyro_sliced[['rms_X', 'rms_Y', 'rms_Z']].values,\n",
    "    gyro_sliced[['skew_X', 'skew_Y', 'skew_Z']].values,\n",
    "    gyro_sliced[['entropy_X', 'entropy_Y', 'entropy_Z']].values,\n",
    "    gyro_sliced[['fft_X', 'fft_Y', 'fft_Z']].values,\n",
    "    gyro_sliced[['peaks_X', 'peaks_Y', 'peaks_Z']].values,\n",
    "    gyro_sliced[['stft_X', 'stft_Y', 'stft_Z']].values\n",
    "))\n",
    "\n",
    "# 라벨 추가 (UCI-HAR 데이터에 라벨링 있음)\n",
    "y_train = pd.read_csv(uci_har_path + 'train/y_train.txt', header=None).values.flatten()\n",
    "\n",
    "# y_train을 X_train_features의 크기에 맞추어 반복 (업샘플링)\n",
    "y_train_upsampled = np.repeat(y_train, 2)  # 레이블을 2배로 확장\n",
    "\n",
    "# 데이터 크기 일치 확인\n",
    "print(f\"X_train_features shape: {X_train_features.shape}\")\n",
    "print(f\"y_train_upsampled shape: {y_train_upsampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c97ded0-3ed9-4e9f-b1ce-254b2de112b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 학습용과 검증용으로 나누기\n",
    "X_train_features, X_val, y_train_upsampled, y_val = train_test_split(X_train_features, \n",
    "                                                                     y_train_upsampled, \n",
    "                                                                     test_size=0.2, \n",
    "                                                                     random_state=42)\n",
    "\n",
    "# 데이터 차원 확장 (batch_size, sequence_length, input_size) 형식으로 맞추기\n",
    "X_train_features = X_train_features.reshape(X_train_features.shape[0], 1, X_train_features.shape[1])\n",
    "X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "\n",
    "# y_train은 그대로 둡니다. 이 변수는 차원 확장이 필요하지 않습니다.\n",
    "y_train = y_train_upsampled.astype(np.int64)\n",
    "y_val = y_val.astype(np.int64)\n",
    "\n",
    "# 데이터셋 및 데이터로더 정의\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_features, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a995c5bb-5f84-44df-afc2-20212fbfa312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 (LSTM, GRU, CNN-LSTM, BiLSTM, Transformer)\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.gru.num_layers, x.size(0), self.gru.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(64, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # CNN에 맞춰 입력 차원 확장\n",
    "        x = self.conv(x.permute(0, 2, 1))\n",
    "        out, _ = self.lstm(x.permute(0, 2, 1))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.bilstm.num_layers * 2, x.size(0), self.bilstm.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.bilstm.num_layers * 2, x.size(0), self.bilstm.hidden_size).to(x.device)\n",
    "        out, _ = self.bilstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_heads, num_layers, num_classes):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.transformer = nn.Transformer(d_model=hidden_size, nhead=num_heads, num_encoder_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        out = self.fc(x[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b332f-4415-4200-8074-fe83e8a4c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_classes 정의\n",
    "num_classes = len(np.unique(y_train))  # y_train 데이터에서 고유한 클래스 수 계산\n",
    "\n",
    "# 레이블 값을 1씩 빼서 0부터 시작하도록 변환\n",
    "y_train = y_train - 1\n",
    "y_val = y_val - 1\n",
    "\n",
    "# 모델 리스트 정의\n",
    "models = [\n",
    "    LSTMModel(input_size=X_train_features.shape[2], hidden_size=128, num_layers=2, num_classes=num_classes),\n",
    "    GRUModel(input_size=X_train_features.shape[2], hidden_size=128, num_layers=2, num_classes=num_classes),\n",
    "    CNNLSTMModel(input_size=X_train_features.shape[2], hidden_size=128, num_layers=2, num_classes=num_classes),\n",
    "    BiLSTMModel(input_size=X_train_features.shape[2], hidden_size=128, num_layers=2, num_classes=num_classes),\n",
    "    TransformerModel(input_size=X_train_features.shape[2], hidden_size=128, num_heads=4, num_layers=2, num_classes=num_classes)\n",
    "]\n",
    "\n",
    "# RandomizedSearchCV 하이퍼파라미터 최적화\n",
    "params = {\n",
    "    'lr': uniform(0.0001, 0.01),\n",
    "    'module__hidden_size': [64, 128, 256],\n",
    "    'module__num_layers': [1, 2, 3],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# 결과 저장\n",
    "best_params_per_model = {}\n",
    "best_scores_per_model = {}\n",
    "\n",
    "# 각 모델에 대해 RandomizedSearchCV 실행\n",
    "for model in models:\n",
    "    print(f\"Optimizing model: {model.__class__.__name__}\")\n",
    "\n",
    "    # NeuralNetClassifier로 모델 래핑\n",
    "    net = NeuralNetClassifier(\n",
    "        module=model,\n",
    "        module__input_size=X_train_features.shape[2],\n",
    "        module__num_classes=num_classes,\n",
    "        criterion=nn.CrossEntropyLoss,\n",
    "        optimizer=optim.Adam,\n",
    "        max_epochs=20,\n",
    "        iterator_train__shuffle=True,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        verbose=1  # 매 에포크마다 로그 출력\n",
    "    )\n",
    "\n",
    "    # RandomizedSearchCV 설정\n",
    "    rs = RandomizedSearchCV(\n",
    "        net,\n",
    "        params,\n",
    "        refit=True,\n",
    "        cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring='accuracy',\n",
    "        n_iter=10,\n",
    "        verbose=2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 모델 최적화\n",
    "    X_train_np = X_train_features.astype(np.float32)  # numpy 형식으로 변환\n",
    "    y_train_np = y_train.astype(np.int64)\n",
    "    \n",
    "    rs.fit(X_train_np, y_train_np)  # numpy 데이터 사용\n",
    "\n",
    "    # 최적 하이퍼파라미터와 점수 저장\n",
    "    best_params_per_model[model.__class__.__name__] = rs.best_params_\n",
    "    best_scores_per_model[model.__class__.__name__] = rs.best_score_\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Best parameters for {model.__class__.__name__}: {rs.best_params_}\")\n",
    "    print(f\"Best cross-validation accuracy for {model.__class__.__name__}: {rs.best_score_:.4f}\")\n",
    "\n",
    "# 최적 모델로 훈련 후 검증 데이터 평가\n",
    "train_acc = net.score(X_train_np, y_train_np)\n",
    "val_acc = net.score(X_val_np, y_val_np)  # X_val_np와 y_val_np도 numpy로 변환 필요\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# 최종 결과 출력\n",
    "print(\"Best parameters for each model:\")\n",
    "print(best_params_per_model)\n",
    "print(\"Best cross-validation accuracy for each model:\")\n",
    "print(best_scores_per_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a46b87d-6a7d-47e6-b6c5-d097fb02baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, train_loader, val_loader, num_epochs, learning_rate):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "    # 마지막 평가 결과\n",
    "    accuracy = accuracy_score(val_correct, val_total)\n",
    "    f1 = f1_score(val_correct, val_total, average='weighted')\n",
    "\n",
    "    return accuracy, f1, train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86393598-fca7-4dc4-b941-c3d6b794e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화\n",
    "def plot_learning_curves(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # 손실 시각화\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # 정확도 시각화\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6505c08-57f9-4f3e-b511-f4338a7acc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 기반 앙상블 예측 함수\n",
    "def weighted_ensemble_predict(models, features, model_f1_scores):\n",
    "    model_predictions = []\n",
    "    weights = []\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            model_predictions.append(predicted.cpu().numpy())\n",
    "            weights.append(model_f1_scores[i])\n",
    "\n",
    "    model_predictions = np.array(model_predictions)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    weighted_predictions = np.zeros(model_predictions.shape[1], dtype=int)\n",
    "    for i in range(model_predictions.shape[1]):\n",
    "        weighted_sum = Counter()\n",
    "        for j in range(model_predictions.shape[0]):\n",
    "            weighted_sum[model_predictions[j, i]] += weights[j]\n",
    "        weighted_predictions[i] = weighted_sum.most_common(1)[0][0]\n",
    "\n",
    "    return weighted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e4730-a740-43d7-8c80-78f0885e5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results = {}\n",
    "for model in models:\n",
    "    accuracy, f1, train_losses, val_losses, train_accuracies, val_accuracies = train_and_evaluate_model(\n",
    "        model, train_loader, val_loader, num_epochs=num_epochs, learning_rate=learning_rate\n",
    "    )\n",
    "    results[model.__class__.__name__] = {'accuracy': accuracy, 'f1': f1}\n",
    "    plot_learning_curves(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "best_model_name = max(results, key=lambda x: results[x]['f1'])\n",
    "best_model = [model for model in models if model.__class__.__name__ == best_model_name][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f95b3b9-d194-4c65-8af5-ae0d9fbb249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 모델 저장 및 불러오기\n",
    "def save_model(model, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "def load_model(model, filename):\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    model.eval()\n",
    "\n",
    "save_model(best_model, './모델/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6be8a-58a3-4e8b-91dd-fffe9e59cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 자이로 데이터 예측 및 라벨링 추가\n",
    "gyro_tensor = torch.tensor(X_gyro_features, dtype=torch.float32)\n",
    "predicted_labels = weighted_ensemble_predict(models, gyro_tensor, model_f1_scores)\n",
    "gyro_sliced['predicted_label'] = predicted_labels\n",
    "gyro_sliced.to_csv('./원본 데이터/자이로 데이터_라벨링.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a884ec-baa7-49d3-9c74-5c7d5d0cabff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 예측 결과 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "gyro_sliced['predicted_label'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Predicted Activity Distribution')\n",
    "plt.xlabel('Activity Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "gyro_sliced['predicted_label_numeric'] = gyro_sliced['predicted_label'].factorize()[0]\n",
    "plt.plot(gyro_sliced['RegisterDate'], gyro_sliced['predicted_label_numeric'], label='Predicted Label')\n",
    "plt.title('Activity Prediction Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Predicted Activity (Numeric)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"자이로 데이터에 예측된 라벨을 추가한 결과 파일이 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
