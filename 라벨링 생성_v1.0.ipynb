{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986b6683-8d4b-452c-999b-7d3600486bcb",
   "metadata": {},
   "source": [
    "> UCI-HAR와 자이로 데이터 간의 차이를 줄이고, 자이로 데이터에 적합한 행동 라벨링을 추가"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ada6549-69b7-49a6-b716-143ccd015520",
   "metadata": {},
   "source": [
    "## 1. 라이브러리 및 데이터셋 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ec6ef79-f095-4874-a7b3-abbde581675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 라이브러리 및 데이터셋 로드\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from scipy.fft import fft\n",
    "from scipy.signal import butter, filtfilt, resample\n",
    "from scipy.stats import skew, entropy\n",
    "\n",
    "# GPU 사용 여부 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038b78be-c034-4b00-b7dd-da7c49f23981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# GPU 사용 여부 확인\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb51601-77a7-425f-86dd-2afdc0e62774",
   "metadata": {},
   "source": [
    "### 1. 저역통과 필터 및 데이터 전처리 함수 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b692459-a690-4403-85cb-ac5ca704d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저역통과 필터 적용\n",
    "def butter_lowpass(cutoff, fs, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def apply_lowpass_filter(data, cutoff, fs):\n",
    "    b, a = butter_lowpass(cutoff, fs)\n",
    "    return filtfilt(b, a, data, axis=0)\n",
    "\n",
    "# 데이터 정규화 및 차원 맞춤 함수\n",
    "def normalize_and_reshape_segments(segments):\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_segments = np.zeros_like(segments)\n",
    "    for axis in range(segments.shape[2]):\n",
    "        normalized_segments[:, :, axis] = scaler.fit_transform(segments[:, :, axis])\n",
    "    return normalized_segments\n",
    "\n",
    "# 슬라이딩 윈도우 생성 함수\n",
    "def create_sliding_windows(data, window_size=128, stride=64):\n",
    "    num_windows = (len(data) - window_size) // stride + 1\n",
    "    windows = np.array([data[i:i+window_size] for i in range(0, len(data) - window_size + 1, stride)])\n",
    "    return windows  # (num_windows, window_size, 3) 형태로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c255f570-ff03-405e-99be-48a07eeba80b",
   "metadata": {},
   "source": [
    "### 2. UCI-HAR 데이터 로드 및 자이로 데이터 전처리 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38683cf5-1585-4ada-838a-95e30cf91ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UCI-HAR 데이터 로드 완료\n",
      "X_train_features 형태: (941056, 15)\n",
      "y_train 형태: (7352, 1)\n"
     ]
    }
   ],
   "source": [
    "# 피처 엔지니어링 함수 정의\n",
    "def calculate_features(df, axis):\n",
    "    # RMS, Skewness, Entropy 계산 (여기에 실제 계산을 추가)\n",
    "    df[f'rms_{axis}'] = np.sqrt(np.mean(df[axis]**2))\n",
    "    df[f'skew_{axis}'] = df[axis].skew()\n",
    "    df[f'entropy_{axis}'] = entropy(np.abs(df[axis]))\n",
    "    return df\n",
    "\n",
    "# FFT 및 STFT 계산 함수 수정\n",
    "def calculate_fft_stft(df, axis, window_size=128):\n",
    "    fft_values = []\n",
    "    \n",
    "    # 각 윈도우에 대해 FFT 계산\n",
    "    for i in range(0, len(df), window_size):\n",
    "        window_data = df[axis].values[i:i+window_size]\n",
    "        if len(window_data) == window_size:\n",
    "            fft_result = np.abs(np.fft.fft(window_data))[:10]  # 절대값 사용\n",
    "            fft_values.append(fft_result)\n",
    "    \n",
    "    if len(fft_values) == 0:\n",
    "        raise ValueError(f\"FFT 계산을 위한 유효한 윈도우가 없습니다. 데이터가 충분한지 확인하세요.\")\n",
    "    \n",
    "    fft_values = np.array(fft_values)\n",
    "    \n",
    "    # FFT 값의 평균을 계산하여 하나의 컬럼으로 추가\n",
    "    df[f'fft_{axis}'] = np.mean(fft_values, axis=0)[0]  # 첫 번째 FFT 값만 사용\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# 정규화 함수 수정\n",
    "def normalize_data(df, columns_to_scale):\n",
    "    scaler = MinMaxScaler()\n",
    "    # 존재하는 컬럼만 정규화\n",
    "    existing_columns = [col for col in columns_to_scale if col in df.columns]\n",
    "    if existing_columns:\n",
    "        df[existing_columns] = scaler.fit_transform(df[existing_columns])\n",
    "    return df\n",
    "\n",
    "# 공통 데이터 처리 함수 정의\n",
    "def preprocess_data(X_data):\n",
    "    gyro_df = pd.DataFrame(X_data.reshape(-1, 3), columns=['X', 'Y', 'Z'])\n",
    "    \n",
    "    # FFT 계산 (피처 엔지니어링 전에 수행)\n",
    "    for axis in ['X', 'Y', 'Z']:\n",
    "        gyro_df = calculate_fft_stft(gyro_df, axis)\n",
    "    \n",
    "    # 피처 엔지니어링 적용\n",
    "    for axis in ['X', 'Y', 'Z']:\n",
    "        gyro_df = calculate_features(gyro_df, axis)\n",
    "    \n",
    "    # 정규화할 컬럼\n",
    "    columns_to_scale = ['fft_X', 'fft_Y', 'fft_Z', 'rms_X', 'rms_Y', 'rms_Z', 'skew_X', 'skew_Y', 'skew_Z', 'entropy_X', 'entropy_Y', 'entropy_Z']\n",
    "    \n",
    "    # 컬럼 존재 여부 확인 및 정규화\n",
    "    gyro_df = normalize_data(gyro_df, columns_to_scale)\n",
    "    \n",
    "    # 최종 피처 세트 구성\n",
    "    feature_columns = ['X', 'Y', 'Z', 'rms_X', 'rms_Y', 'rms_Z', 'skew_X', 'skew_Y', 'skew_Z', 'entropy_X', 'entropy_Y', 'entropy_Z', 'fft_X', 'fft_Y', 'fft_Z']\n",
    "    X_features = gyro_df[feature_columns].values\n",
    "    \n",
    "    return X_features\n",
    "\n",
    "# UCI-HAR 데이터 로드 함수\n",
    "def load_ucihar_data():\n",
    "    uci_har_path = './원본 데이터/UCI HAR Dataset/'\n",
    "    gyro_x_train = pd.read_csv(uci_har_path + 'train/Inertial Signals/body_gyro_x_train.txt', sep='\\s+', header=None).values\n",
    "    gyro_y_train = pd.read_csv(uci_har_path + 'train/Inertial Signals/body_gyro_y_train.txt', sep='\\s+', header=None).values\n",
    "    gyro_z_train = pd.read_csv(uci_har_path + 'train/Inertial Signals/body_gyro_z_train.txt', sep='\\s+', header=None).values\n",
    "    labels_train = pd.read_csv(uci_har_path + 'train/y_train.txt', sep='\\s+', header=None).values - 1\n",
    "    \n",
    "    # X 데이터를 결합하여 피처 엔지니어링 수행\n",
    "    X_train = np.stack([gyro_x_train, gyro_y_train, gyro_z_train], axis=-1)\n",
    "    X_train_features = preprocess_data(X_train)\n",
    "    \n",
    "    return X_train_features, labels_train\n",
    "\n",
    "# 자이로 데이터 전처리 함수\n",
    "def preprocess_gyro_data(gyro_path):\n",
    "    gyro_data = pd.read_csv(gyro_path)\n",
    "    X_data = gyro_data[['X', 'Y', 'Z']].values\n",
    "    X_train_features = preprocess_data(X_data)\n",
    "    \n",
    "    # 텐서로 변환\n",
    "    gyro_data_tensor = torch.tensor(X_train_features, dtype=torch.float32)\n",
    "    return gyro_data_tensor\n",
    "\n",
    "# UCI-HAR 데이터 로드 및 전처리\n",
    "X_train_features, y_train = load_ucihar_data()\n",
    "print(\"UCI-HAR 데이터 로드 완료\")\n",
    "print(f\"X_train_features 형태: {X_train_features.shape}\")\n",
    "print(f\"y_train 형태: {y_train.shape}\")\n",
    "\n",
    "# 자이로 데이터 전처리\n",
    "gyro_data_tensor = preprocess_gyro_data('./원본 데이터/자이로 데이터.csv')\n",
    "print(\"자이로 데이터 전처리 완료\")\n",
    "print(f\"gyro_data_tensor 형태: {gyro_data_tensor.shape}\")\n",
    "\n",
    "# 데이터를 Tensor로 변환\n",
    "X_train_tensor = torch.tensor(X_train_features, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "print(\"데이터 Tensor 변환 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caacb8c2-1abb-4da7-bf34-b2c9715a6ac7",
   "metadata": {},
   "source": [
    "### 3. 데이터 증강 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16230532-a664-4583-8406-8c07e54762a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강\n",
    "def add_gaussian_noise(data, noise_factor=0.05):\n",
    "    noise = torch.randn_like(data) * noise_factor\n",
    "    return data + noise\n",
    "\n",
    "def time_warp(data, sigma=0.2, knot=4):\n",
    "    if len(data.shape) == 2:\n",
    "        data = data.unsqueeze(1)  # (샘플 수, 1, 특성) 형태로 변경\n",
    "    \n",
    "    orig_steps = torch.arange(data.shape[1], dtype=torch.float32)\n",
    "    random_warps = torch.normal(mean=1.0, std=sigma, size=(data.shape[0], knot+2, data.shape[2]))\n",
    "    warp_steps = (torch.ones((data.shape[2],1))*(torch.linspace(0, data.shape[1]-1., knot+2))).t()\n",
    "    ret = torch.zeros_like(data)\n",
    "    for i, pat in enumerate(data):\n",
    "        time_warp = torch.interp(orig_steps, warp_steps[:, 0], random_warps[i, :, 0])\n",
    "        scale = (data.shape[1]-1)/torch.sum(time_warp)\n",
    "        ret[i] = torch.interp(orig_steps, torch.cumsum(time_warp, 0)*scale, pat)\n",
    "    return ret.squeeze(1) if ret.shape[1] == 1 else ret\n",
    "\n",
    "# X_train_tensor 형태 확인 및 출력\n",
    "print(f\"X_train_tensor 원본 형태: {X_train_tensor.shape}\")\n",
    "\n",
    "# 데이터 증강 후 train_test_split 실행\n",
    "# 학습/검증 데이터 분리\n",
    "try:\n",
    "    # 텐서를 NumPy 배열로 변환\n",
    "    X_train_augmented_np = X_train_augmented.cpu().numpy()\n",
    "    y_train_augmented_np = y_train_augmented.cpu().numpy()\n",
    "\n",
    "    # train_test_split 사용\n",
    "    X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(\n",
    "        X_train_augmented_np,\n",
    "        y_train_augmented_np,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_train_augmented_np\n",
    "    )\n",
    "\n",
    "    # 다시 텐서로 변환\n",
    "    X_train_tensor = torch.tensor(X_train_np, dtype=torch.float32).to(device)\n",
    "    X_val_tensor = torch.tensor(X_val_np, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train_np, dtype=torch.long).to(device)\n",
    "    y_val_tensor = torch.tensor(y_val_np, dtype=torch.long).to(device)\n",
    "\n",
    "    print(\"학습/검증 데이터 분리 완료\")\n",
    "    print(f\"X_train_tensor 형태: {X_train_tensor.shape}\")\n",
    "    print(f\"X_val_tensor 형태: {X_val_tensor.shape}\")\n",
    "\n",
    "    # 데이터셋 및 DataLoader 생성\n",
    "    train_dataset = UCIHARData(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    val_dataset = UCIHARData(X_val_tensor, y_val_tensor)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    print(\"DataLoader 생성 완료\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"오류 발생: {e}\")\n",
    "    print(f\"X_train_augmented 형태: {X_train_augmented.shape if 'X_train_augmented' in locals() else 'Not defined'}\")\n",
    "    print(f\"y_train_augmented 형태: {y_train_augmented.shape if 'y_train_augmented' in locals() else 'Not defined'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f66a65-bfa7-44a0-ace2-918bdc495d51",
   "metadata": {},
   "source": [
    "### 4. 데이터셋 및 모델 정의 ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18092f1-b7c1-48a4-8e10-a8a817b40926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 클래스\n",
    "class UCIHARData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx].view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba567f6-5e53-4da3-bf91-2be92ef4f88c",
   "metadata": {},
   "source": [
    "## 5. LSTM, GRU, CNN-LSTM, BiLSTM, Transformer 모델 정의 및 성능 개선(앙상블)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa2695-7fae-44c3-8b44-089e15fde875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 정의\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.input_size = input_size  # 추가\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.batch_norm(out[:, -1, :])\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# GRU 모델 정의\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.input_size = input_size  # 추가\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = self.batch_norm(out[:, -1, :])\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# CNN-LSTM 모델 정의\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.input_size = input_size  # 추가\n",
    "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.lstm = nn.LSTM(64, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# BiLSTM 모델 정의\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.input_size = input_size  # 추가\n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob, bidirectional=True)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size * 2)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.bilstm(x)\n",
    "        out = self.batch_norm(out[:, -1, :])\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Transformer 모델 정의\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, num_heads, num_layers, num_classes, dropout_prob):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_size = input_size  # 추가\n",
    "        self.input_embedding = nn.Linear(input_size, 128)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 180, 128))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=num_heads, dropout=dropout_prob)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, input_size) -> (seq_len, batch_size, input_size)\n",
    "        x = self.input_embedding(x)\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.transformer(x.permute(1, 0, 2))  # (seq_len, batch_size, d_model)\n",
    "        out = self.fc(x[-1, :, :])  # 마지막 타임스텝의 출력만 사용\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c7682c-24fa-402d-ab3a-6b4b4bd2cb76",
   "metadata": {},
   "source": [
    "## 6. 앙상블 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ab5040-8752-4085-8c10-735abab8b5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력 크기 정의\n",
    "input_size = 3  # X, Y, Z 축\n",
    "\n",
    "# 모델 정의\n",
    "models = [\n",
    "    LSTMModel(input_size=input_size, hidden_size=128, num_layers=3, num_classes=6, dropout_prob=0.5).to(device),\n",
    "    GRUModel(input_size=input_size, hidden_size=128, num_layers=3, num_classes=6, dropout_prob=0.5).to(device),\n",
    "    CNNLSTMModel(input_size=input_size, hidden_size=128, num_layers=3, num_classes=6, dropout_prob=0.5).to(device),\n",
    "    BiLSTMModel(input_size=input_size, hidden_size=128, num_layers=3, num_classes=6, dropout_prob=0.5).to(device),\n",
    "    TransformerModel(input_size=input_size, num_heads=4, num_layers=2, num_classes=6, dropout_prob=0.5).to(device)\n",
    "]\n",
    "\n",
    "# 입력 크기 확인\n",
    "print(f\"Input size: {input_size}\")\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"Model {i+1} input size: {model.input_size if hasattr(model, 'input_size') else 'N/A'}\")\n",
    "\n",
    "# 모델들의 예측값을 앙상블로 처리\n",
    "# 앙상블 예측 함수\n",
    "def ensemble_predict(models, X_unlabeled, weights=[1, 1, 1, 1, 1]):\n",
    "    predictions = []\n",
    "    X_unlabeled = X_unlabeled.to(device)\n",
    "    for i, model in enumerate(models):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(X_unlabeled)\n",
    "            predictions.append(weights[i] * torch.softmax(outputs, dim=1))\n",
    "    \n",
    "    final_predictions = torch.argmax(sum(predictions), dim=1)\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8c17d8-8100-4c69-8a9e-9d0e9c4a11b0",
   "metadata": {},
   "source": [
    "## 7. 손실 함수, 옵티마이저, 스케줄러 및 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ee838-0a4f-4a46-8a69-f5973facad92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 클래스별 가중치 계산\n",
    "class_weights = 1.0 / torch.tensor(class_sample_count, dtype=torch.float32)\n",
    "class_weights = class_weights.to(device)\n",
    "\n",
    "# 손실 함수 정의 (가중치 적용)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# 각 모델별로 옵티마이저와 스케줄러 정의\n",
    "optimizers = [\n",
    "    torch.optim.Adam(model.parameters(), lr=0.000001, weight_decay=1e-4) for model in models\n",
    "]\n",
    "\n",
    "schedulers = [\n",
    "    torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0.0001) for optimizer in optimizers\n",
    "]\n",
    "\n",
    "# 모델 저장 함수\n",
    "def save_checkpoint(epoch, model, optimizer, filename=\"checkpoint.pth.tar\"):\n",
    "    state = {'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}\n",
    "    torch.save(state, filename)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "# 모델 학습 함수\n",
    "def train_model(models, train_loader, val_loader, criterion, optimizers, schedulers, num_epochs, patience):\n",
    "    global best_val_accuracy\n",
    "    early_stop_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        for model in models:\n",
    "            model.train()\n",
    "        \n",
    "        running_loss = [0.0] * len(models)\n",
    "        correct = [0] * len(models)\n",
    "        total = 0\n",
    "        \n",
    "        # Training loop\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "           # 수정 코드\n",
    "            labels = labels.to(device).squeeze()  # 1D로 변환\n",
    "            total += labels.size(0)\n",
    "\n",
    "            # 각 모델별 학습\n",
    "            for j, model in enumerate(models):\n",
    "                optimizers[j].zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # 그래디언트 클리핑 추가\n",
    "                optimizers[j].step()\n",
    "                \n",
    "                running_loss[j] += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                correct[j] += (predicted.cpu() == labels.cpu()).sum().item()\n",
    "\n",
    "        # Training accuracy and loss\n",
    "        for j, model in enumerate(models):\n",
    "            accuracy = 100 * correct[j] / total\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Model {j+1} Training Loss: {running_loss[j]/len(train_loader):.4f}, Training Accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        # Validation loop\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "        \n",
    "        val_loss = [0.0] * len(models)\n",
    "        val_correct = [0] * len(models)\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                # 수정 코드\n",
    "                labels = labels.to(device).squeeze()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "                for j, model in enumerate(models):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss[j] += loss.item()\n",
    "\n",
    "                    _, predicted = torch.max(outputs, 1)\n",
    "                    val_correct[j] += (predicted.cpu() == labels.cpu()).sum().item()\n",
    "\n",
    "        # Validation accuracy and loss\n",
    "        for j, model in enumerate(models):\n",
    "            val_accuracy = (val_correct[j] / val_total) * 100\n",
    "            val_loss[j] = val_loss[j] / len(val_loader)\n",
    "\n",
    "            print(f\"Model {j+1} Validation Accuracy: {val_accuracy:.2f}%, Validation Loss: {val_loss[j]:.4f}\")\n",
    "\n",
    "            # 스케줄러 갱신\n",
    "            schedulers[j].step()\n",
    "\n",
    "            # Save best model\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                save_checkpoint(epoch, model, optimizers[j], filename=f\"best_model_{j+1}.pth.tar\")\n",
    "                print(f\"Best model {j+1} saved with accuracy: {best_val_accuracy:.2f}%\")\n",
    "                early_stop_counter = 0\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if early_stop_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# 모델 학습 실행\n",
    "train_model(models, train_loader, val_loader, criterion, optimizers, schedulers, num_epochs=1000, patience=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ffcde-3ec3-4776-92b9-cc004933b884",
   "metadata": {},
   "source": [
    "## 8. 슬라이딩 윈도우와 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976f6b35-b35d-4ca7-9adf-1b3f540d83ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 학습된 모델 로드\n",
    "for i, model in enumerate(models):\n",
    "    checkpoint = torch.load(f'best_model_{i+1}.pth.tar')\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "# 자이로 데이터를 GPU로 이동\n",
    "gyro_data_tensor = gyro_data_tensor.to(device)\n",
    "\n",
    "print(f\"gyro_data_tensor shape: {gyro_data_tensor.shape}\")\n",
    "\n",
    "# 예측\n",
    "batch_size = 32\n",
    "all_predictions = []\n",
    "\n",
    "for i in range(0, len(gyro_data_tensor), batch_size):\n",
    "    batch = gyro_data_tensor[i:i+batch_size]\n",
    "    # 마지막 배치 처리\n",
    "    if len(batch) < batch_size:\n",
    "        padding = torch.zeros(batch_size - len(batch), *batch.shape[1:], device=device)\n",
    "        batch = torch.cat([batch, padding], dim=0)\n",
    "    batch_predictions = ensemble_predict(models, batch)\n",
    "    # 패딩 제거\n",
    "    all_predictions.append(batch_predictions[:len(gyro_data_tensor[i:i+batch_size])])\n",
    "\n",
    "all_predictions = torch.cat(all_predictions).cpu().numpy()\n",
    "\n",
    "# 슬라이딩 윈도우 예측 결과를 원본 데이터에 매핑하는 함수\n",
    "def map_predictions_to_original(original_data_length, predictions, window_size, stride):\n",
    "    mapped_predictions = np.zeros((original_data_length, 6))  # 6은 활동 클래스의 수\n",
    "    counts = np.zeros(original_data_length)\n",
    "    \n",
    "    for i, pred in enumerate(predictions):\n",
    "        start = i * stride\n",
    "        end = min(start + window_size, original_data_length)\n",
    "        mapped_predictions[start:end, pred] += 1\n",
    "        counts[start:end] += 1\n",
    "    \n",
    "    # 각 데이터 포인트에 대해 가장 많이 예측된 라벨 선택\n",
    "    final_predictions = np.argmax(mapped_predictions, axis=1)\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "# 원본 자이로 데이터 로드\n",
    "original_gyro_data = pd.read_csv('./원본 데이터/자이로 데이터.csv')\n",
    "\n",
    "# 원본 데이터에 맞는 예측 라벨 생성\n",
    "original_predictions = map_predictions_to_original(len(original_gyro_data), all_predictions, window_size=128, stride=64)\n",
    "\n",
    "# 예측된 라벨을 활동 이름으로 매핑\n",
    "activity_labels = {0: \"walking\", 1: \"walking_upstairs\", 2: \"walking_downstairs\", 3: \"sitting\", 4: \"standing\", 5: \"laying\"}\n",
    "predicted_activities = [activity_labels[label] for label in original_predictions]\n",
    "\n",
    "# 예측 결과 출력\n",
    "for i, activity in enumerate(predicted_activities[:10]):\n",
    "    print(f\"Sample {i}: {activity}\")\n",
    "\n",
    "# 자이로 데이터에 예측 결과 추가\n",
    "original_gyro_data['Predicted_Activity'] = predicted_activities\n",
    "\n",
    "# 결과 저장\n",
    "original_gyro_data.to_csv('./원본 데이터/자이로 데이터_예측.csv', index=False)\n",
    "\n",
    "# 예측 결과 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "activity_counts = pd.Series(predicted_activities).value_counts()\n",
    "sns.barplot(x=activity_counts.index, y=activity_counts.values)\n",
    "plt.title('Predicted Activity Distribution')\n",
    "plt.xlabel('Activity')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 시간에 따른 활동 변화 시각화\n",
    "plt.figure(figsize=(20, 6))\n",
    "activity_series = pd.Series(predicted_activities)\n",
    "activity_numeric = pd.factorize(activity_series)[0]\n",
    "plt.plot(activity_numeric)\n",
    "plt.title('Activity Changes Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Activity')\n",
    "plt.yticks(range(len(activity_labels)), list(activity_labels.values()))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"예측 완료 및 결과 저장됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a38691-f8b2-4b16-aeba-9f04c24c1a15",
   "metadata": {},
   "source": [
    "## 9. 성능 평가 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ff3ec-f240-460e-a936-6e29681dd6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자이로 데이터에 라벨 추가\n",
    "gyro_data = pd.read_csv('./원본 데이터/자이로 데이터.csv')\n",
    "gyro_data['Predicted_Activity'] = predicted_activities[:len(gyro_data)]  # 길이 맞춤\n",
    "\n",
    "# 라벨링된 자이로 데이터를 저장\n",
    "gyro_data.to_csv('./원본 데이터/자이로 데이터_예측.csv', index=False)\n",
    "\n",
    "# 예측된 활동별 분포 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x=gyro_data['Predicted_Activity'])\n",
    "plt.title('Predicted Activity Distribution')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# 예측 결과 요약\n",
    "print(gyro_data['Predicted_Activity'].value_counts(normalize=True))\n",
    "\n",
    "# 클러스터링 적용 및 결과 비교\n",
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "kmeans_labels = kmeans.fit_predict(gyro_data_tensor.cpu().numpy().reshape(gyro_data_tensor.shape[0], -1))\n",
    "\n",
    "# 클러스터링 결과와 실제 예측 라벨 비교\n",
    "kmeans_accuracy = accuracy_score(all_predictions, kmeans_labels)\n",
    "print(f\"K-Means Clustering Accuracy: {kmeans_accuracy:.2f}\")\n",
    "\n",
    "# Confusion Matrix 및 성능 평가\n",
    "cm = confusion_matrix(all_predictions, kmeans_labels)\n",
    "ConfusionMatrixDisplay(cm).plot()\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(all_predictions, kmeans_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
