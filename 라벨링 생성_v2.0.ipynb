{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "si3P9_YaIapq"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.fft import fft\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import resample\n",
    "from scipy.stats import skew, kurtosis, entropy, uniform\n",
    "from scipy.signal import find_peaks, stft\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from skorch import NeuralNetClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import Counter\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4zBnHoEIefM"
   },
   "source": [
    "# **1. 데이터 로드 및 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PevDI1lsIliA",
    "outputId": "e8a7d64a-0d4e-430e-8295-fcf76fb3c579"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gyro X train shape: (7352, 128)\n",
      "Gyro Y train shape: (7352, 128)\n",
      "Gyro Z train shape: (7352, 128)\n"
     ]
    }
   ],
   "source": [
    "# 자이로 데이터 로드 (RegisterDate를 datetime으로 변환)\n",
    "gyro_data = pd.read_csv('./원본 데이터/자이로 데이터.csv')\n",
    "gyro_data['RegisterDate'] = pd.to_datetime(gyro_data['RegisterDate'])\n",
    "\n",
    "# UCI-HAR ZIP 파일 경로 설정\n",
    "uci_har_zip_path = './원본 데이터/UCI HAR Dataset.zip'\n",
    "\n",
    "# ZIP 파일 열기\n",
    "with zipfile.ZipFile(uci_har_zip_path, 'r') as zip_ref:\n",
    "    # ZIP 파일에서 원하는 파일들을 추출하지 않고 바로 읽을 수 있습니다.\n",
    "\n",
    "    # X, Y, Z 축 자이로스코프 데이터를 각 축별로 파일에서 불러옴\n",
    "    with zip_ref.open('UCI HAR Dataset/train/Inertial Signals/body_gyro_x_train.txt') as gyro_x_file:\n",
    "        gyro_x_train = pd.read_csv(gyro_x_file, sep='\\s+', header=None).values\n",
    "\n",
    "    with zip_ref.open('UCI HAR Dataset/train/Inertial Signals/body_gyro_y_train.txt') as gyro_y_file:\n",
    "        gyro_y_train = pd.read_csv(gyro_y_file, sep='\\s+', header=None).values\n",
    "\n",
    "    with zip_ref.open('UCI HAR Dataset/train/Inertial Signals/body_gyro_z_train.txt') as gyro_z_file:\n",
    "        gyro_z_train = pd.read_csv(gyro_z_file, sep='\\s+', header=None).values\n",
    "\n",
    "# 확인\n",
    "print(f\"Gyro X train shape: {gyro_x_train.shape}\")\n",
    "print(f\"Gyro Y train shape: {gyro_y_train.shape}\")\n",
    "print(f\"Gyro Z train shape: {gyro_z_train.shape}\")\n",
    "\n",
    "# UCI-HAR 데이터의 각 축별 평균값으로 DataFrame 생성\n",
    "uci_har_gyro_df = pd.DataFrame({\n",
    "    'X': np.mean(gyro_x_train, axis=1),\n",
    "    'Y': np.mean(gyro_y_train, axis=1),\n",
    "    'Z': np.mean(gyro_z_train, axis=1)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n455RtpUIgb8"
   },
   "source": [
    "# **2. 주파수 업샘플링 (UCI-HAR 데이터를 50Hz에서 100Hz로 업샘플링)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "U1BkZ0UKIrNU"
   },
   "outputs": [],
   "source": [
    "current_freq = 50  # 원본 UCI-HAR 데이터 주파수 (50Hz)\n",
    "desired_freq = 100  # 자이로 데이터 주파수 (100Hz)\n",
    "\n",
    "# 업샘플링을 위한 시간축 생성\n",
    "t_current = np.linspace(0, len(uci_har_gyro_df) / current_freq, num=len(uci_har_gyro_df))\n",
    "t_new = np.linspace(0, len(uci_har_gyro_df) / current_freq, num=len(uci_har_gyro_df) * 2)\n",
    "\n",
    "# 각 축에 대해 선형 보간법을 사용한 업샘플링\n",
    "uci_har_gyro_df_upsampled = pd.DataFrame({\n",
    "    'X': np.interp(t_new, t_current, uci_har_gyro_df['X']),\n",
    "    'Y': np.interp(t_new, t_current, uci_har_gyro_df['Y']),\n",
    "    'Z': np.interp(t_new, t_current, uci_har_gyro_df['Z'])\n",
    "})\n",
    "\n",
    "# 자이로 데이터에서 UCI-HAR 데이터 길이에 맞춰 슬라이싱\n",
    "n_samples_uci = len(uci_har_gyro_df)\n",
    "gyro_sliced = gyro_data.iloc[:n_samples_uci].copy()  # UCI-HAR 데이터 길이만큼 자이로 데이터를 슬라이싱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GA3VjeggIq3a"
   },
   "source": [
    "# **3. 데이터 정규화**\n",
    "***\n",
    "> MinMaxScaler를 사용하여 자이로 데이터와 UCI-HAR 데이터를 [0,1] 범위로 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "JFGIxsuZI0z-"
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "gyro_sliced[['X', 'Y', 'Z']] = scaler.fit_transform(gyro_sliced[['X', 'Y', 'Z']])\n",
    "uci_har_gyro_df[['X', 'Y', 'Z']] = scaler.fit_transform(uci_har_gyro_df[['X', 'Y', 'Z']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dZtEozbnI2TV"
   },
   "source": [
    "# **4. 특성 엔지니어링 함수 정의**\n",
    "***\n",
    "> RMS(root mean square, 제곱평균제곱근), Skewness(왜도), Kurtosis(첨도), Entropy(불확실성) 및 피크 탐지 계산 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "IoJLWy6CI4DK"
   },
   "outputs": [],
   "source": [
    "# RMS 함수 정의\n",
    "def rms(values):\n",
    "    return np.sqrt(np.mean(values**2))\n",
    "\n",
    "# 엔트로피 계산 함수 정의\n",
    "def calc_entropy(values):\n",
    "    # 확률 밀도 함수 계산 후 엔트로피 계산\n",
    "    value_prob = np.histogram(values, bins=30, density=True)[0]  # 확률 밀도 함수\n",
    "    return entropy(value_prob + 1e-6)  # 엔트로피 계산\n",
    "\n",
    "# FFT 특징 계산 함수\n",
    "def fft_features(values, n=10):\n",
    "    fft_vals = np.abs(fft(values))  # FFT 계산 후 절댓값을 취함\n",
    "    return np.mean(fft_vals[:n])  # 주파수 성분의 상위 N개 평균 계산\n",
    "\n",
    "# STFT 특징 계산 함수\n",
    "def stft_features(values, n=10):\n",
    "    _, _, Zxx = stft(values)\n",
    "    Zxx_flat = np.abs(Zxx).flatten()  # STFT 결과를 1차원으로 변환\n",
    "    Zxx_mean = np.mean(Zxx_flat)  # 플랫한 결과의 평균값을 계산\n",
    "    return Zxx_mean\n",
    "\n",
    "# 각 축별로 RMS, Skewness, Kurtosis, Entropy 및 피크 탐지를 계산\n",
    "def calculate_features(df, axis):\n",
    "    df[f'rms_{axis}'] = rms(df[axis])\n",
    "    df[f'skew_{axis}'] = skew(df[axis])\n",
    "    df[f'kurtosis_{axis}'] = kurtosis(df[axis])\n",
    "    df[f'entropy_{axis}'] = calc_entropy(df[axis])\n",
    "\n",
    "    # 피크 탐지\n",
    "    peaks, _ = find_peaks(df[axis], height=0)\n",
    "    df[f'peaks_{axis}'] = 0\n",
    "    df.loc[peaks, f'peaks_{axis}'] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "# 푸리에 변환 (FFT) 및 STFT 계산 함수\n",
    "def calculate_fft_stft(df, axis, n=10):\n",
    "    # FFT 계산\n",
    "    df[f'fft_{axis}'] = fft_features(df[axis].values, n=n)\n",
    "\n",
    "    # STFT 계산\n",
    "    stft_result = stft_features(df[axis].values)\n",
    "    df[f'stft_{axis}'] = stft_result\n",
    "\n",
    "    return df\n",
    "\n",
    "# 모든 축(X, Y, Z)에 대해 위에서 정의한 특성 계산\n",
    "for axis in ['X', 'Y', 'Z']:\n",
    "    uci_har_gyro_df_upsampled = calculate_features(uci_har_gyro_df_upsampled, axis)\n",
    "    gyro_sliced = calculate_features(gyro_sliced, axis)\n",
    "    uci_har_gyro_df_upsampled = calculate_fft_stft(uci_har_gyro_df_upsampled, axis)\n",
    "    gyro_sliced = calculate_fft_stft(gyro_sliced, axis)\n",
    "\n",
    "# 데이터 정규화 (MinMaxScaler)\n",
    "scaler = MinMaxScaler()\n",
    "# 컬럼 이름의 대소문자를 일치시킵니다.\n",
    "columns_to_scale = ['fft_X', 'fft_Y', 'fft_Z', 'stft_X', 'stft_Y', 'stft_Z']\n",
    "uci_har_gyro_df_upsampled[columns_to_scale] = scaler.fit_transform(uci_har_gyro_df_upsampled[columns_to_scale])\n",
    "gyro_sliced[columns_to_scale] = scaler.fit_transform(gyro_sliced[columns_to_scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KRXwj6dHI_AS"
   },
   "source": [
    "# 5. **특성 데이터 구축 및 학습 데이터 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MLzoWLUnJBpw",
    "outputId": "3180a4a9-9549-4ef4-bb56-412c874de1ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (7352,)\n"
     ]
    }
   ],
   "source": [
    "# 최종 피처 세트 구축\n",
    "X_train_features = np.column_stack((\n",
    "    uci_har_gyro_df_upsampled[['X', 'Y', 'Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['rms_X', 'rms_Y', 'rms_Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['skew_X', 'skew_Y', 'skew_Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['entropy_X', 'entropy_Y', 'entropy_Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['fft_X', 'fft_Y', 'fft_Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['peaks_X', 'peaks_Y', 'peaks_Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['stft_X', 'stft_Y', 'stft_Z']].values\n",
    "))\n",
    "\n",
    "X_gyro_features = np.column_stack((\n",
    "    gyro_sliced[['X', 'Y', 'Z']].values,\n",
    "    gyro_sliced[['rms_X', 'rms_Y', 'rms_Z']].values,\n",
    "    gyro_sliced[['skew_X', 'skew_Y', 'skew_Z']].values,\n",
    "    gyro_sliced[['entropy_X', 'entropy_Y', 'entropy_Z']].values,\n",
    "    gyro_sliced[['fft_X', 'fft_Y', 'fft_Z']].values,\n",
    "    gyro_sliced[['peaks_X', 'peaks_Y', 'peaks_Z']].values,\n",
    "    gyro_sliced[['stft_X', 'stft_Y', 'stft_Z']].values\n",
    "))\n",
    "\n",
    "# ZIP 파일 열기\n",
    "with zipfile.ZipFile(uci_har_zip_path, 'r') as zip_ref:\n",
    "    # ZIP 파일 내의 y_train.txt 파일을 불러옴\n",
    "    with zip_ref.open('UCI HAR Dataset/train/y_train.txt') as y_train_file:\n",
    "        y_train = pd.read_csv(y_train_file, header=None).values.flatten()\n",
    "\n",
    "# 확인\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "\n",
    "# 라벨 데이터에서 최소값이 1이므로, 모든 값을 1씩 감소시켜 0부터 시작하도록 변환\n",
    "y_train_upsampled = np.repeat(y_train, 2)  # 레이블을 2배로 확장\n",
    "if y_train_upsampled.min() > 0:  # 라벨 값이 1부터 시작하면 1씩 감소\n",
    "    y_train_upsampled = y_train_upsampled - 1\n",
    "\n",
    "# 데이터셋을 학습용과 검증용으로 나누기\n",
    "X_train_features, X_val, y_train_upsampled, y_val = train_test_split(X_train_features,\n",
    "                                                                     y_train_upsampled,\n",
    "                                                                     test_size=0.2,\n",
    "                                                                     random_state=42)\n",
    "\n",
    "# 검증 세트의 라벨도 같은 방식으로 변환\n",
    "if y_val.min() > 0:  # 라벨 값이 1부터 시작하면 1씩 감소\n",
    "    y_val = y_val - 1\n",
    "\n",
    "# 데이터 차원 확장 (batch_size, sequence_length, input_size)\n",
    "X_train_features = X_train_features.reshape(X_train_features.shape[0], 1, X_train_features.shape[1])\n",
    "X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "\n",
    "# TensorDataset 및 DataLoader 정의\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_features, dtype=torch.float32), torch.tensor(y_train_upsampled, dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkKRU8DtJMkF"
   },
   "source": [
    "# **6. 모델 정의 (LSTM, GRU, CNN-LSTM, BiLSTM, Transformer)**\n",
    "***\n",
    "> 각 모델은 행동 인지 분류를 수행하기 위한 다른 신경망 구조로 정의됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_MbADJiaJIkU"
   },
   "outputs": [],
   "source": [
    "# 모델 정의 (LSTM, GRU, CNN-LSTM, BiLSTM, Transformer)\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.gru.num_layers, x.size(0), self.gru.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=21, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(64, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: (batch_size, seq_length, input_size)\n",
    "        x = x.permute(0, 2, 1)  # Change to (batch_size, input_size, seq_length) for conv1d\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0, 2, 1)  # Change back to (batch_size, seq_length, out_channels)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.bilstm.num_layers * 2, x.size(0), self.bilstm.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.bilstm.num_layers * 2, x.size(0), self.bilstm.hidden_size).to(x.device)\n",
    "        out, _ = self.bilstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_heads, num_layers, num_classes, dropout):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, input_size, hidden_size))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x.shape: (batch_size, seq_length, input_size)\n",
    "        x = self.embedding(x) + self.positional_encoding\n",
    "        x = self.transformer(x)\n",
    "        out = self.fc(x[:, -1, :])  # 마지막 시점의 출력만 사용\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKYfXpVZJGj4"
   },
   "source": [
    "# **7. 모델 학습 및 평가**\n",
    "***\n",
    "> 선택한 모델을 사용해 학습 및 검증 데이터를 학습시키고, 최적의 모델을 찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "JlVXS0Q7JO5T"
   },
   "outputs": [],
   "source": [
    "# 학습 결과를 로그 파일에 저장하는 함수 추가\n",
    "def log_training_results(train_losses, val_losses, train_accuracies, val_accuracies, filename='training_log.txt'):\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(\"Epoch\\tTrain Loss\\tVal Loss\\tTrain Accuracy\\tVal Accuracy\\n\")\n",
    "        for epoch, (train_loss, val_loss, train_acc, val_acc) in enumerate(zip(train_losses, val_losses, train_accuracies, val_accuracies), 1):\n",
    "            f.write(f\"{epoch}\\t{train_loss:.4f}\\t{val_loss:.4f}\\t{train_acc:.2f}%\\t{val_acc:.2f}%\\n\")\n",
    "\n",
    "# 체크포인트 저장 함수 추가\n",
    "def save_checkpoint(model, optimizer, epoch, filename):\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "# 체크포인트 불러오기 함수 추가\n",
    "def load_checkpoint(filename, model, optimizer):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    return model, optimizer, epoch\n",
    "\n",
    "# EarlyStopping 클래스 추가\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# 모델 학습 및 평가 함수 정의\n",
    "def train_and_evaluate_model(model, train_loader, val_loader, num_epochs, learning_rate):\n",
    "    criterion = nn.CrossEntropyLoss()  # 분류 문제이므로 CrossEntropyLoss 사용\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)  # Adam Optimizer 사용\n",
    "    early_stopping = EarlyStopping(patience=3, min_delta=0.01)  # Early Stopping 설정\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = []\n",
    "\n",
    "    # 체크포인트를 로드하여 학습을 이어서 진행\n",
    "    try:\n",
    "        model, optimizer, start_epoch = load_checkpoint('checkpoint_v2.0.pth', model, optimizer)\n",
    "        print(f\"Continuing training from epoch {start_epoch}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No checkpoint found, starting from epoch 0\")\n",
    "        start_epoch = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        # 학습 과정\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # 에포크마다 체크포인트 저장\n",
    "        save_checkpoint(model, optimizer, epoch, f'checkpoint_epoch_{epoch}.pth')\n",
    "\n",
    "        # 에포크별 손실 및 정확도 계산\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        # 검증 과정\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # 검증 데이터의 손실 및 정확도 계산\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "        # Early stopping 체크\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # 마지막 평가 결과\n",
    "    accuracy = accuracy_score(val_correct, val_total)\n",
    "    f1 = f1_score(val_correct, val_total, average='weighted')\n",
    "\n",
    "    # 3. 모델 학습이 끝난 후 로그 저장 추가\n",
    "    log_training_results(train_losses, val_losses, train_accuracies, val_accuracies, 'training_log.txt')\n",
    "\n",
    "    return accuracy, f1, train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Prlp3NQJT2f"
   },
   "source": [
    "# **8. 학습 곡선 시각화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ytctvnRfJV3t"
   },
   "outputs": [],
   "source": [
    "def plot_learning_curves(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # 손실 시각화\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # 정확도 시각화\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fnSzjNgCJX3U"
   },
   "source": [
    "# **9.모델 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "54CQf4amJZWt",
    "outputId": "b6d0e463-6980-40fd-8b2f-c9d93c41b2bd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# num_classes 정의\n",
    "num_classes = len(np.unique(y_train))  # y_train 데이터에서 고유한 클래스 수 계산\n",
    "\n",
    "# 모델 리스트 정의\n",
    "models = [\n",
    "    LSTMModel(input_size=X_train_features.shape[2], hidden_size=128, num_layers=2, num_classes=num_classes, dropout=0.3),\n",
    "    GRUModel(input_size=X_train_features.shape[2], hidden_size=128, num_layers=2, num_classes=num_classes, dropout=0.3),\n",
    "    CNNLSTMModel(input_size=X_train_features.shape[2], hidden_size=128, num_layers=2, num_classes=num_classes, dropout=0.3),\n",
    "    BiLSTMModel(input_size=X_train_features.shape[2], hidden_size=128, num_layers=2, num_classes=num_classes, dropout=0.3),\n",
    "    TransformerModel(input_size=X_train_features.shape[2], hidden_size=128, num_heads=4, num_layers=2, num_classes=num_classes, dropout=0.3)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecKOYCj9Jax4"
   },
   "source": [
    "# **10. RandomizedSearchCV 하이퍼파라미터 최적화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "OWlFLReSJdVP",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c4a864f7-296d-45af-f98b-5f6f94c29738"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing model: LSTMModel\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.7720\u001b[0m       \u001b[32m0.2545\u001b[0m        \u001b[35m1.7219\u001b[0m  2.2033\n",
      "      2        \u001b[36m1.6143\u001b[0m       \u001b[32m0.2853\u001b[0m        \u001b[35m1.5507\u001b[0m  1.8249\n",
      "      3        \u001b[36m1.5427\u001b[0m       \u001b[32m0.3071\u001b[0m        \u001b[35m1.5142\u001b[0m  1.7082\n",
      "      4        \u001b[36m1.5162\u001b[0m       \u001b[32m0.3321\u001b[0m        \u001b[35m1.4900\u001b[0m  1.2988\n",
      "      5        \u001b[36m1.4951\u001b[0m       0.3151        \u001b[35m1.4832\u001b[0m  1.3342\n",
      "      6        \u001b[36m1.4786\u001b[0m       \u001b[32m0.3480\u001b[0m        1.4875  1.3481\n",
      "      7        \u001b[36m1.4659\u001b[0m       0.3172        \u001b[35m1.4601\u001b[0m  1.3021\n",
      "      8        \u001b[36m1.4582\u001b[0m       0.3092        \u001b[35m1.4434\u001b[0m  1.9335\n",
      "      9        \u001b[36m1.4495\u001b[0m       0.3204        \u001b[35m1.4376\u001b[0m  2.5328\n",
      "     10        \u001b[36m1.4374\u001b[0m       0.3459        \u001b[35m1.4128\u001b[0m  4.7385\n",
      "     11        \u001b[36m1.4353\u001b[0m       0.3247        1.4252  3.9928\n",
      "     12        \u001b[36m1.4285\u001b[0m       0.3401        \u001b[35m1.3932\u001b[0m  5.6597\n",
      "     13        \u001b[36m1.4183\u001b[0m       \u001b[32m0.3629\u001b[0m        \u001b[35m1.3921\u001b[0m  13.2294\n",
      "     14        \u001b[36m1.4168\u001b[0m       0.3374        1.4273  15.0697\n",
      "     15        \u001b[36m1.4145\u001b[0m       \u001b[32m0.3704\u001b[0m        1.3962  15.2042\n",
      "     16        \u001b[36m1.4068\u001b[0m       0.3629        1.3939  15.1126\n",
      "     17        \u001b[36m1.4011\u001b[0m       \u001b[32m0.3751\u001b[0m        \u001b[35m1.3789\u001b[0m  15.0750\n",
      "     18        \u001b[36m1.3967\u001b[0m       0.3714        \u001b[35m1.3669\u001b[0m  16.4522\n",
      "     19        \u001b[36m1.3900\u001b[0m       \u001b[32m0.3847\u001b[0m        \u001b[35m1.3612\u001b[0m  15.9778\n",
      "     20        1.3900       0.3645        1.3649  15.3184\n",
      "[CV] END batch_size=64, lr=0.008065429868602328, module__dropout=0.5, module__hidden_size=256, module__num_layers=1, optimizer__weight_decay=0.0001; total time= 2.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.7801\u001b[0m       \u001b[32m0.2981\u001b[0m        \u001b[35m1.7486\u001b[0m  1.8621\n",
      "      2        \u001b[36m1.6443\u001b[0m       \u001b[32m0.3018\u001b[0m        \u001b[35m1.5526\u001b[0m  1.2799\n",
      "      3        \u001b[36m1.5361\u001b[0m       0.2933        \u001b[35m1.5408\u001b[0m  1.3080\n",
      "      4        \u001b[36m1.5067\u001b[0m       \u001b[32m0.3055\u001b[0m        \u001b[35m1.4830\u001b[0m  1.3413\n",
      "      5        \u001b[36m1.4904\u001b[0m       \u001b[32m0.3241\u001b[0m        \u001b[35m1.4779\u001b[0m  1.3424\n",
      "      6        \u001b[36m1.4759\u001b[0m       0.2869        1.4787  1.2955\n",
      "      7        \u001b[36m1.4759\u001b[0m       0.3151        \u001b[35m1.4667\u001b[0m  1.3081\n",
      "      8        \u001b[36m1.4532\u001b[0m       \u001b[32m0.3273\u001b[0m        \u001b[35m1.4336\u001b[0m  1.3053\n",
      "      9        \u001b[36m1.4467\u001b[0m       \u001b[32m0.3422\u001b[0m        \u001b[35m1.4221\u001b[0m  1.5420\n",
      "     10        \u001b[36m1.4405\u001b[0m       \u001b[32m0.3698\u001b[0m        1.4232  1.7812\n",
      "     11        \u001b[36m1.4301\u001b[0m       0.3459        \u001b[35m1.4164\u001b[0m  2.4854\n",
      "     12        \u001b[36m1.4289\u001b[0m       0.3518        1.4176  4.2289\n",
      "     13        \u001b[36m1.4233\u001b[0m       0.3682        1.4374  13.0660\n",
      "     14        \u001b[36m1.4166\u001b[0m       0.3640        \u001b[35m1.3990\u001b[0m  15.1292\n",
      "     15        \u001b[36m1.4157\u001b[0m       0.3688        \u001b[35m1.3932\u001b[0m  15.1821\n",
      "     16        \u001b[36m1.4021\u001b[0m       \u001b[32m0.3820\u001b[0m        \u001b[35m1.3832\u001b[0m  15.1774\n",
      "     17        \u001b[36m1.3992\u001b[0m       0.3385        \u001b[35m1.3826\u001b[0m  15.2055\n",
      "     18        1.4024       0.3581        \u001b[35m1.3801\u001b[0m  15.1006\n",
      "     19        \u001b[36m1.3938\u001b[0m       0.3778        \u001b[35m1.3773\u001b[0m  15.1743\n",
      "     20        \u001b[36m1.3914\u001b[0m       0.3666        \u001b[35m1.3738\u001b[0m  15.1654\n",
      "[CV] END batch_size=64, lr=0.008065429868602328, module__dropout=0.5, module__hidden_size=256, module__num_layers=1, optimizer__weight_decay=0.0001; total time= 2.3min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.7831\u001b[0m       \u001b[32m0.2317\u001b[0m        \u001b[35m1.7584\u001b[0m  1.3076\n",
      "      2        \u001b[36m1.6390\u001b[0m       \u001b[32m0.2816\u001b[0m        \u001b[35m1.5591\u001b[0m  1.3114\n",
      "      3        \u001b[36m1.5510\u001b[0m       \u001b[32m0.3108\u001b[0m        \u001b[35m1.5104\u001b[0m  1.7831\n",
      "      4        \u001b[36m1.5082\u001b[0m       0.2880        \u001b[35m1.5010\u001b[0m  1.8141\n",
      "      5        \u001b[36m1.4864\u001b[0m       \u001b[32m0.3278\u001b[0m        \u001b[35m1.4971\u001b[0m  1.8711\n",
      "      6        \u001b[36m1.4769\u001b[0m       0.2875        \u001b[35m1.4859\u001b[0m  1.2881\n",
      "      7        \u001b[36m1.4694\u001b[0m       0.3193        \u001b[35m1.4727\u001b[0m  1.3051\n",
      "      8        \u001b[36m1.4626\u001b[0m       \u001b[32m0.3496\u001b[0m        \u001b[35m1.4389\u001b[0m  1.3007\n",
      "      9        \u001b[36m1.4495\u001b[0m       0.3464        \u001b[35m1.4294\u001b[0m  1.2733\n",
      "     10        \u001b[36m1.4421\u001b[0m       0.3459        \u001b[35m1.4218\u001b[0m  1.2914\n",
      "     11        \u001b[36m1.4330\u001b[0m       0.3353        1.4245  1.9486\n",
      "     12        1.4337       \u001b[32m0.3937\u001b[0m        1.4350  4.8849\n",
      "     13        \u001b[36m1.4182\u001b[0m       0.3603        \u001b[35m1.3975\u001b[0m  12.9941\n",
      "     14        \u001b[36m1.4090\u001b[0m       0.3518        \u001b[35m1.3905\u001b[0m  17.0134\n",
      "     15        1.4100       0.3549        \u001b[35m1.3876\u001b[0m  15.9551\n",
      "     16        \u001b[36m1.4007\u001b[0m       0.2981        1.4342  15.2983\n",
      "     17        \u001b[36m1.3998\u001b[0m       0.3677        \u001b[35m1.3837\u001b[0m  15.3300\n",
      "     18        \u001b[36m1.3939\u001b[0m       0.3385        1.3940  15.3515\n",
      "     19        \u001b[36m1.3896\u001b[0m       0.3629        \u001b[35m1.3715\u001b[0m  15.1895\n",
      "     20        \u001b[36m1.3852\u001b[0m       0.3496        1.3731  15.1857\n",
      "[CV] END batch_size=64, lr=0.008065429868602328, module__dropout=0.5, module__hidden_size=256, module__num_layers=1, optimizer__weight_decay=0.0001; total time= 2.4min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:88: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.7744\u001b[0m       \u001b[32m0.2862\u001b[0m        \u001b[35m1.7319\u001b[0m  1.7517\n",
      "      2        \u001b[36m1.6247\u001b[0m       \u001b[32m0.2931\u001b[0m        \u001b[35m1.5499\u001b[0m  1.3013\n",
      "      3        \u001b[36m1.5370\u001b[0m       0.2894        1.5838  1.2914\n",
      "      4        \u001b[36m1.5162\u001b[0m       \u001b[32m0.3075\u001b[0m        \u001b[35m1.5057\u001b[0m  1.2420\n",
      "      5        \u001b[36m1.4922\u001b[0m       \u001b[32m0.3208\u001b[0m        1.5197  1.2629\n",
      "      6        \u001b[36m1.4838\u001b[0m       0.3181        \u001b[35m1.4708\u001b[0m  1.2677\n",
      "      7        \u001b[36m1.4667\u001b[0m       \u001b[32m0.3425\u001b[0m        \u001b[35m1.4703\u001b[0m  1.2940\n",
      "      8        \u001b[36m1.4601\u001b[0m       \u001b[32m0.3452\u001b[0m        \u001b[35m1.4397\u001b[0m  1.2410\n",
      "      9        \u001b[36m1.4493\u001b[0m       \u001b[32m0.3473\u001b[0m        1.4531  1.5062\n",
      "     10        \u001b[36m1.4399\u001b[0m       0.3468        \u001b[35m1.4342\u001b[0m  1.7814\n",
      "     11        \u001b[36m1.4326\u001b[0m       \u001b[32m0.3563\u001b[0m        \u001b[35m1.4191\u001b[0m  2.4781\n",
      "     12        \u001b[36m1.4274\u001b[0m       \u001b[32m0.3654\u001b[0m        1.4253  4.1196\n",
      "     13        \u001b[36m1.4194\u001b[0m       0.3633        \u001b[35m1.4105\u001b[0m  13.2215\n",
      "     14        \u001b[36m1.4112\u001b[0m       0.3595        1.4116  15.2045\n"
     ]
    }
   ],
   "source": [
    "# 결과 저장\n",
    "best_params_per_model = {}\n",
    "best_scores_per_model = {}\n",
    "\n",
    "# 각 모델에 대해 RandomizedSearchCV 실행\n",
    "for model in models:\n",
    "    print(f\"Optimizing model: {model.__class__.__name__}\")\n",
    "\n",
    "    if isinstance(model, TransformerModel):\n",
    "        # TransformerModel에 필요한 하이퍼파라미터 설정\n",
    "        params = {\n",
    "            'lr': uniform(0.0001, 0.01),\n",
    "            'module__hidden_size': [64, 128, 256],\n",
    "            'module__num_layers': [1, 2, 3],\n",
    "            'module__num_heads': [4, 8],  # Transformer에만 필요한 num_heads\n",
    "            'module__dropout': [0.2, 0.3, 0.5],\n",
    "            'optimizer__weight_decay': [1e-4, 1e-5, 0],\n",
    "            'batch_size': [16, 32, 64]\n",
    "        }\n",
    "    else:\n",
    "        # LSTM, GRU, CNN-LSTM 등 다른 모델에 필요한 하이퍼파라미터 설정\n",
    "        params = {\n",
    "            'lr': uniform(0.0001, 0.01),\n",
    "            'module__hidden_size': [64, 128, 256],\n",
    "            'module__num_layers': [1, 2, 3],\n",
    "            'module__dropout': [0.2, 0.3, 0.5],\n",
    "            'optimizer__weight_decay': [1e-4, 1e-5, 0],\n",
    "            'batch_size': [16, 32, 64]\n",
    "        }\n",
    "\n",
    "    # NeuralNetClassifier로 모델 래핑\n",
    "    net = NeuralNetClassifier(\n",
    "        module=model,\n",
    "        module__input_size=X_train_features.shape[2],\n",
    "        module__num_classes=num_classes,\n",
    "        criterion=nn.CrossEntropyLoss,\n",
    "        optimizer=optim.Adam,\n",
    "        max_epochs=20,\n",
    "        iterator_train__shuffle=True,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        verbose=1  # 매 에포크마다 로그 출력\n",
    "    )\n",
    "\n",
    "    # TensorDataset을 사용하기 전에, 학습 데이터를 NumPy 배열로 변환\n",
    "    X_train_np = X_train_features.astype(np.float32)  # X_train_features를 NumPy 배열로 변환\n",
    "    y_train_np = y_train_upsampled.astype(np.int64)   # y_train_upsampled을 NumPy 배열로 변환\n",
    "\n",
    "    # RandomizedSearchCV 설정\n",
    "    rs = RandomizedSearchCV(\n",
    "        net,\n",
    "        params,\n",
    "        refit=True,\n",
    "        cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring='accuracy',\n",
    "        n_iter=10,\n",
    "        verbose=2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        rs.fit(X_train_np, y_train_np)\n",
    "    except RuntimeError as e:\n",
    "        print(f\"Error occurred during model fitting: {e}\")\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        # GPU 관련 에러가 있을 때 CPU로 전환\n",
    "        if \"CUDA\" in str(e):\n",
    "            net.set_params(device='cpu')\n",
    "            rs.fit(X_train_np, y_train_np)\n",
    "\n",
    "    # 모델 학습 후 메모리 해제\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()  # GPU 메모리 초기화\n",
    "\n",
    "    # 최적 하이퍼파라미터와 점수 저장\n",
    "    best_params_per_model[model.__class__.__name__] = rs.best_params_\n",
    "    best_scores_per_model[model.__class__.__name__] = rs.best_score_\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Best parameters for {model.__class__.__name__}: {rs.best_params_}\")\n",
    "    print(f\"Best cross-validation accuracy for {model.__class__.__name__}: {rs.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-06ImLXJgTA"
   },
   "source": [
    "# **11. 최적의 성능을 가진 모델을 선택**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hEb9wlb2Jkhx"
   },
   "outputs": [],
   "source": [
    "# 최고의 성능 모델을 선택하는 기준을 튜플로 설정\n",
    "results = {}\n",
    "model_f1_scores = []\n",
    "\n",
    "for model in models:\n",
    "    accuracy, f1, train_losses, val_losses, train_accuracies, val_accuracies = train_and_evaluate_model(\n",
    "        model, train_loader, val_loader, num_epochs=num_epochs, learning_rate=learning_rate\n",
    "    )\n",
    "    results[model.__class__.__name__] = {'accuracy': accuracy, 'f1': f1}\n",
    "    model_f1_scores.append(f1)  # F1 점수를 저장하여 가중치 앙상블에 사용\n",
    "    plot_learning_curves(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "# F1 및 Accuracy 기준으로 최적 모델 선택\n",
    "best_model_name = max(results, key=lambda x: (results[x]['f1'], results[x]['accuracy']))\n",
    "best_model = [model for model in models if model.__class__.__name__ == best_model_name][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYVezcAdJmIC"
   },
   "source": [
    "# **12. 최적 모델로 훈련 후 검증 데이터 평가**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hlvC7V6oJpK5"
   },
   "outputs": [],
   "source": [
    "X_val_np = X_val.astype(np.float32)\n",
    "y_val_np = y_val.astype(np.int64)\n",
    "\n",
    "train_acc = net.score(X_train_np, y_train_np)\n",
    "val_acc = net.score(X_val_np, y_val_np)\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMslRY4oJqjA"
   },
   "source": [
    "# **13. 모델 저장 및 불러오기**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zt4SPYG3JsJk"
   },
   "outputs": [],
   "source": [
    "# 최적 모델 저장 함수\n",
    "def save_model(model, filename):\n",
    "    # 학습된 모델의 상태를 파일로 저장\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "# 학습된 최적의 모델을 저장\n",
    "save_model(best_model, './모델/best_model.pth')\n",
    "\n",
    "# 모델 불러오기 함수\n",
    "def load_model(model, filename):\n",
    "    # 저장된 모델의 상태를 불러와서 현재 모델에 적용\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    model.eval()  # 평가 모드로 전환 (추론을 위해)\n",
    "\n",
    "# 저장된 최적 모델 불러오기\n",
    "load_model(best_model, './모델/best_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqMTl_zAJ0h_"
   },
   "source": [
    "# **14. 가중치 기반 앙상블 예측**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NRs7lWz6J1NR"
   },
   "outputs": [],
   "source": [
    "# 가중치 기반 앙상블 예측 함수 정의\n",
    "def weighted_ensemble_predict(models, features, model_f1_scores):\n",
    "    model_predictions = []\n",
    "    weights = []\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            model_predictions.append(predicted.cpu().numpy())\n",
    "            weights.append(model_f1_scores[i])\n",
    "\n",
    "    model_predictions = np.array(model_predictions)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    weighted_predictions = np.zeros(model_predictions.shape[1], dtype=int)\n",
    "    for i in range(model_predictions.shape[1]):\n",
    "        weighted_sum = Counter()\n",
    "        for j in range(model_predictions.shape[0]):\n",
    "            weighted_sum[model_predictions[j, i]] += weights[j]\n",
    "        weighted_predictions[i] = weighted_sum.most_common(1)[0][0]\n",
    "\n",
    "    return weighted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f9RB0MZJ2t7"
   },
   "outputs": [],
   "source": [
    "# 불러온 모델로 예측 수행\n",
    "gyro_tensor = torch.tensor(X_gyro_features, dtype=torch.float32)\n",
    "with torch.no_grad():\n",
    "    predicted_labels = best_model(gyro_tensor).argmax(dim=1)\n",
    "    gyro_sliced['predicted_label'] = predicted_labels.cpu().numpy()\n",
    "\n",
    "# 결과 저장\n",
    "gyro_sliced.to_csv('./원본 데이터/자이로 데이터_라벨링.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKFZutF8J4Lp"
   },
   "outputs": [],
   "source": [
    "# 예측 결과 시각화\n",
    "gyro_tensor = torch.tensor(X_gyro_features, dtype=torch.float32)\n",
    "predicted_labels = weighted_ensemble_predict(models, gyro_tensor, model_f1_scores)\n",
    "\n",
    "gyro_sliced['predicted_label'] = predicted_labels\n",
    "\n",
    "gyro_sliced.to_csv('./원본 데이터/자이로 데이터_라벨링.csv', index=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "gyro_sliced['predicted_label'].value_counts().sort_index().plot(kind='bar') ㅊ\n",
    "\n",
    "# 시간에 따른 예측된 활동 분포 시각화\n",
    "plt.figure(figsize=(20, 8))\n",
    "gyro_sliced['predicted_label_numeric'] = gyro_sliced['predicted_label'].factorize()[0]\n",
    "plt.plot(gyro_sliced['RegisterDate'], gyro_sliced['predicted_label_numeric'], label='Predicted Label')\n",
    "plt.title('Activity Prediction Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Predicted Activity (Numeric)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"자이로 데이터에 예측된 라벨을 추가한 결과 파일이 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
