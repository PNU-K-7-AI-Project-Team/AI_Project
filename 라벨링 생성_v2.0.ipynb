{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96da23fb-9120-456f-becf-99891c6c6496",
   "metadata": {},
   "source": [
    "전체 코드 흐름:\n",
    "1. 라이브러리 및 데이터셋 로드\n",
    "2. 데이터 전처리: 저역통과 필터, 슬라이딩 윈도우 생성, 정규화\n",
    "3. 데이터 증강 및 학습/검증 데이터 분리\n",
    "4. 데이터셋 정의 및 DataLoader 설정\n",
    "5. 모델 정의 (LSTM, GRU, CNN-LSTM, Transformer)\n",
    "6. 모델 학습 및 검증\n",
    "7. 라벨 예측 및 자이로 데이터에 라벨 추가\n",
    "8. 결과 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1689a3-4181-463c-a37a-32f1bb029012",
   "metadata": {},
   "source": [
    "# 1. 라이브러리 및 데이터셋 로드\n",
    "***\n",
    "> 필요한 라이브러리를 로드하고 GPU 사용 여부를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd57aef3-cdad-487f-acd8-86d71922cea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "# 1. 라이브러리 및 데이터셋 로드\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from scipy.signal import butter, filtfilt, resample\n",
    "\n",
    "# GPU 사용 여부 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# GPU 사용 여부 확인\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03379b7-1eaa-4149-929f-445c6a5207bc",
   "metadata": {},
   "source": [
    "# 2. 데이터 전처리: 저역통과 필터, 다운샘플링, 슬라이딩 윈도우 적용\n",
    "> 자이로 데이터를 다운샘플링하고 노이즈 제거를 위한 저역통과 필터를 적용\n",
    "> 슬라이딩 윈도우를 통해 데이터를 모델이 학습하기 적합한 형태로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d8e23c5-e76c-4e71-96d4-72614896004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저역통과 필터 적용\n",
    "def butter_lowpass(cutoff, fs, order=4):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "# 필터 적용 함수\n",
    "def apply_lowpass_filter(data, cutoff, fs):\n",
    "    b, a = butter_lowpass(cutoff, fs)\n",
    "    return filtfilt(b, a, data,axis=0)\n",
    "\n",
    "# 데이터 정규화 및 차원 맞춤 함수\n",
    "def normalize_and_reshape_segments(segments):\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_segments = np.zeros_like(segments)\n",
    "    for axis in range(segments.shape[2]):\n",
    "        normalized_segments[:, :, axis] = scaler.fit_transform(segments[:, :, axis])\n",
    "    return normalized_segments\n",
    "\n",
    "# 슬라이딩 윈도우 생성 함수\n",
    "def create_sliding_windows(data, window_size, stride):  # 2초 윈도우, 0.5초 stride \n",
    "    num_windows = (len(data) - window_size) // stride + 1\n",
    "    windows = np.array([data[i : i + window_size] for i in range(0, len(data) - window_size + 1, stride)])\n",
    "    return windows  # (num_windows, window_size, 3) 형태로 반환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d659dc-abf5-4ac8-9aa3-a9cdacaf9ce3",
   "metadata": {},
   "source": [
    "# 3. UCI-HAR 데이터 및 자이로 데이터 전처리\n",
    "***\n",
    "> UCI-HAR 데이터를 로드하고 자이로 데이터를 동일하게 전처리\n",
    "> 샘플링 주파수를 맞추고 저역통과 필터를 적용하여 노이즈를 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32ea57c-e050-4092-acfc-ad364b011e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCI-HAR 데이터 로드\n",
    "def load_ucihar_data():\n",
    "    ucihar_path = './원본 데이터/UCI HAR Dataset/'\n",
    "    gyro_x_train = pd.read_csv(ucihar_path + 'train/Inertial Signals/body_gyro_x_train.txt', sep='\\s+', header=None).values\n",
    "    gyro_y_train = pd.read_csv(ucihar_path + 'train/Inertial Signals/body_gyro_y_train.txt', sep='\\s+', header=None).values\n",
    "    gyro_z_train = pd.read_csv(ucihar_path + 'train/Inertial Signals/body_gyro_z_train.txt', sep='\\s+', header=None).values\n",
    "    labels_train = pd.read_csv(ucihar_path + 'train/y_train.txt', sep='\\s+', header=None).values - 1\n",
    "\n",
    "    X_train = np.stack([gyro_x_train, gyro_y_train, gyro_z_train], axis=-1)\n",
    "    X_train_normalized = normalize_and_reshape_segments(X_train)\n",
    "\n",
    "    return X_train_normalized, labels_train\n",
    "\n",
    "X_train, y_train = load_ucihar_data()\n",
    "\n",
    "# 자이로 데이터 전처리 함수\n",
    "def preprocess_gyro_data(gyro_path, current_freq=100, desired_freq=50, window_size=128, stride=64):\n",
    "    gyro_data = pd.read_csv(gyro_path)\n",
    "\n",
    "    n_samples = int(len(gyro_data) * desired_freq / current_freq)\n",
    "\n",
    "    gyro_data_resampled = resample(gyro_data[['X', 'Y', 'Z']].values, n_samples)\n",
    "    gyro_data_filtered = apply_lowpass_filter(gyro_data_resampled, cutoff=20, fs=desired_freq)\n",
    "    gyro_data_windows = create_sliding_windows(gyro_data_filtered, window_size, stride)\n",
    "    gyro_data_normalized = normalize_and_reshape_segments(gyro_data_windows)\n",
    "    \n",
    "    return torch.tensor(gyro_data_normalized, dtype=torch.float32)\n",
    "\n",
    "gyro_data_tensor = preprocess_gyro_data('./원본 데이터/자이로 데이터.csv')\n",
    "\n",
    "# 데이터를 Tensor로 변환\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "gyro_data_tensor = gyro_data_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ff0d7-de33-4965-b8ff-6b9c67883890",
   "metadata": {},
   "source": [
    "# 4. 데이터 증강\n",
    "***\n",
    "> 학습 데이터의 양을 늘리기 위해 증강 기법으로 가우시안 노이즈, Time Warping, Scaling을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef950901-1b77-43f9-93fb-e883bf881e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 증강 함수: 가우시안 노이즈 추가\n",
    "def add_gaussian_noise(data, noise_factor=0.05):\n",
    "    noise = np.random.randn(*data.shape)\n",
    "    augmented_data = data + noise_factor * noise\n",
    "    augmented_data = np.clip(augmented_data, -1.0, 1.0)\n",
    "    return augmented_data\n",
    "\n",
    "# Time Warping 함수\n",
    "def time_warp(data, sigma=0.2, knot=4):\n",
    "    orig_steps = np.arange(data.shape[1])\n",
    "    random_warps = np.random.normal(loc=1.0, scale=sigma, size=(data.shape[0], knot + 2, data.shape[2]))\n",
    "    warp_steps = ( np.ones((data.shape[2], 1)) * (np.linspace(0, data.shape[1] - 1., num=knot + 2)) ).T\n",
    "    ret = np.zeros_like(data)\n",
    "\n",
    "    for i, pat in enumerate(data):\n",
    "        warper = np.interp(orig_steps, warp_steps[:, 0], random_warps[i, :, 0])\n",
    "        for dim in range(data.shape[2]):\n",
    "            ret[i, :, dim] = np.interp(orig_steps, np.cumsum(warper), pat[:, dim])\n",
    "            \n",
    "    return ret\n",
    "\n",
    "# Scaling 함수\n",
    "def scale(data, sigma=0.1):\n",
    "    scaling_factor = np.random.normal(loc=1.0, scale=sigma, size=(data.shape[0], 1, data.shape[2]))\n",
    "    return data * scaling_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e78f1e-e4cb-49bf-84ad-fcb322c32dd3",
   "metadata": {},
   "source": [
    "# 5. 증강된 데이터 생성 및 학습/검증 데이터 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3e36dc1-c8e9-4739-a716-7ce038a96656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UCI-HAR 데이터셋 및 증강 데이터 생성\n",
    "def create_augmented_data(X_train, y_train):\n",
    "    # 데이터 증강\n",
    "    X_train_augmented = np.concatenate([\n",
    "        X_train,\n",
    "        add_gaussian_noise(X_train),\n",
    "        time_warp(X_train),\n",
    "        scale(X_train)\n",
    "    ], axis=0)\n",
    "    \n",
    "    y_train_augmented = np.concatenate([y_train] * 4, axis=0)\n",
    "    \n",
    "    return X_train_augmented, y_train_augmented\n",
    "\n",
    "# 학습/검증 데이터 분리\n",
    "def split_data(X_augmented, y_augmented):\n",
    "    X_train_tensor, X_val_tensor, y_train_tensor, y_val_tensor = train_test_split(\n",
    "        torch.tensor(X_augmented, dtype=torch.float32),\n",
    "        torch.tensor(y_augmented, dtype=torch.long),\n",
    "        test_size = 0.2, \n",
    "        random_state = 42\n",
    "    )\n",
    "\n",
    "    return X_train_tensor, X_val_tensor, y_train_tensor, y_val_tensor\n",
    "\n",
    "# 증강된 데이터를 학습 및 검증 데이터로 분리\n",
    "X_train_augmented, y_train_augmented = create_augmented_data(X_train, y_train)\n",
    "X_train_tensor, X_val_tensor, y_train_tensor, y_val_tensor = split_data(X_train_augmented, y_train_augmented)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7db7dfd-4b24-4a70-9a91-28d526a45df8",
   "metadata": {},
   "source": [
    "# 6. 데이터셋 정의\n",
    "***\n",
    "> 증강된 데이터를 학습에 사용할 수 있도록 `Dataset`으로 정의하고\n",
    "> 모델을 학습시키기 위한 다양한 시계열 모델(LSTM, GRU, CNN-LSTM, Transformer 등)을 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8679071-a09d-49a5-9a88-cf3a80b1f20a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: torch.Size([23526, 128, 3]), Labels shape: torch.Size([23526, 1])\n",
      "Validation data shape: torch.Size([5882, 128, 3]), Labels shape: torch.Size([5882, 1])\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 클래스\n",
    "class UCIHARData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx].view(-1)\n",
    "\n",
    "# 학습 및 검증 데이터 로더 정의\n",
    "train_dataset = UCIHARData(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = UCIHARData(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# 확인 출력\n",
    "print(f'Training data shape: {X_train_tensor.shape}, Labels shape: {y_train_tensor.shape}')\n",
    "print(f'Validation data shape: {X_val_tensor.shape}, Labels shape: {y_val_tensor.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152130ff-8ae2-4669-ba3f-1efcb417b485",
   "metadata": {},
   "source": [
    "# 7.모델 정의 (LSTM, GRU, CNN-LSTM, Transformer)\n",
    "***\n",
    "> 시계열 데이터를 처리할 수 있는 다양한 모델을 정의\n",
    "> 각 모델은 LSTM, GRU, CNN-LSTM, Transformer의 구조를 가지고 있으며, 동일한 데이터셋에서 성능을 비교 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cac37e7b-8eed-4450-bac4-07475fb2b267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM 모델 정의\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.batch_norm(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# GRU 모델 정의\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        out = self.batch_norm(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# CNN-LSTM 모델 정의\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_size, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        self.lstm = nn.LSTM(64, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Transformer 모델 정의\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, num_heads, num_layers, num_classes, dropout_prob):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_embedding = nn.Linear(input_size, 128)\n",
    "        self.positional_encoding = nn.Parameter(torch.randn(1, 200, 128))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=128, nhead=num_heads, dropout=dropout_prob)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.fc = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.input_embedding(x)\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = self.transformer(x.permute(1, 0, 2))\n",
    "        out = self.fc(x[-1, :, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f7db01-76ea-4a34-ad08-f9050f48a818",
   "metadata": {},
   "source": [
    "# 8. 모델 학습 및 검증\n",
    "***\n",
    "> 각 모델을 학습하고 검증 데이터로 평가한 후, 최적의 모델을 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b27ff8dc-9cbc-4046-9fd1-d05073abd2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def get_optimizer(model, learning_rate = 0.001):\n",
    "    return torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def get_scheduler(optimizer, step_size=10, gamma=0.1):\n",
    "    return torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# 모델 학습 함수\n",
    "def train_model(model, train_loader, optimizer, criterion, scheduler, num_epochs=20):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).squeeze()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        scheduler.step()\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_accuracy = 100 * correct / total\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%')\n",
    "\n",
    "    return epoch_accuracy\n",
    "\n",
    "# 모델 평가 함수\n",
    "def evaluate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device).squeeze()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    val_loss = running_loss / len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "    return val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc55387-08e3-46dd-be29-1214de2cbec2",
   "metadata": {},
   "source": [
    "# 9. 최적 모델 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c91ec4ce-986b-4dc8-88d1-aaa7cabd0f15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hiddensize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_model\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# 모델 리스트\u001b[39;00m\n\u001b[0;32m     26\u001b[0m models \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 27\u001b[0m     \u001b[43mLSTMModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_prob\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     28\u001b[0m     GRUModel(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, dropout_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     29\u001b[0m     CNNLSTMModel(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, dropout_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device),\n\u001b[0;32m     30\u001b[0m     TransformerModel(input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, dropout_prob\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     31\u001b[0m ]\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 최적의 모델 선택\u001b[39;00m\n\u001b[0;32m     34\u001b[0m best_model \u001b[38;5;241m=\u001b[39m train_and_select_best_model(models, train_loader, val_loader)\n",
      "Cell \u001b[1;32mIn[7], line 5\u001b[0m, in \u001b[0;36mLSTMModel.__init__\u001b[1;34m(self, input_size, hidden_size, num_layers, num_classes, dropout_prob)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_size, hidden_size, num_layers, num_classes, dropout_prob):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28msuper\u001b[39m(LSTMModel, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLSTM(input_size, \u001b[43mhiddensize\u001b[49m, num_layers, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, dropout\u001b[38;5;241m=\u001b[39mdropout_prob)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBatchNorm1d(hidden_size)\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(hidden_size, num_classes)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'hiddensize' is not defined"
     ]
    }
   ],
   "source": [
    "# 여러 모델 학습 및 평가 후 가장 좋은 모델을 선택\n",
    "def train_and_select_best_model(models, train_loader, val_loader, num_epochs=20):\n",
    "    best_accuracy = 0.0\n",
    "    best_model = None\n",
    "\n",
    "    for model in models:\n",
    "        optimizer = get_optimizer(model)\n",
    "        scheduler = get_scheduler(optimizer)\n",
    "        print(f\"Training {model.__class__.__name__}...\")\n",
    "\n",
    "        # 모델 학습\n",
    "        train_model(model, train_loader, optimizer, criterion, scheduler, num_epochs)\n",
    "\n",
    "        # 검증 데이터로 평가\n",
    "        val_accuracy = evaluate_model(model, val_loader, criterion)\n",
    "\n",
    "        # 가장 좋은 성능을 보인 모델을 선택\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_model = model\n",
    "\n",
    "    print(f\"Best model: {best_model.__class__.__name__}, Accuracy: {best_accuracy:.2f}%\")\n",
    "    return best_model\n",
    "\n",
    "# 모델 리스트\n",
    "models = [\n",
    "    LSTMModel(input_size=3, hidden_size=128, num_layers=2, num_classes=6, dropout_prob=0.5).to(device),\n",
    "    GRUModel(input_size=3, hidden_size=128, num_layers=2, num_classes=6, dropout_prob=0.5).to(device),\n",
    "    CNNLSTMModel(input_size=3, hidden_size=128, num_layers=2, num_classes=6, dropout_prob=0.5).to(device),\n",
    "    TransformerModel(input_size=3, num_heads=4, num_layers=2, num_classes=6, dropout_prob=0.5).to(device)\n",
    "]\n",
    "\n",
    "# 최적의 모델 선택\n",
    "best_model = train_and_select_best_model(models, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f0b95f-eced-42a2-958f-a229b9d6b884",
   "metadata": {},
   "source": [
    "# 10. 라벨 예측 및 자이로 데이터로 라벨 추가\n",
    "***\n",
    "> 최적의 모델을 선택하여 자이로 데이터에 라벨을 예측하고, 원본 데이터에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77127cb1-a82e-4412-b382-0e8b7d437af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측된 라벨을 자이로 데이터에 매핑하는 함수\n",
    "def map_predictions_to_original(original_data_length, predictions, window_size=128, stride=64):\n",
    "    mapped_predictions = np.zeros((original_data_length, 6))  # 6은 활동 클래스의 수\n",
    "    counts = np.zeros(original_data_length)\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        start = i * stride\n",
    "        end = min(start + window_size, original_data_length)\n",
    "        mapped_predictions[start:end, pred] += 1\n",
    "        counts[start:end] += 1\n",
    "\n",
    "    final_predictions = np.argmax(mapped_predictions, axis=1)\n",
    "\n",
    "    return final_predictions\n",
    "\n",
    "# 최종 예측 수행\n",
    "def predict_labels(model, gyro_data_tensor):\n",
    "    model.eval()\n",
    "\n",
    "    all_predictions = []\n",
    "    batch_size = 100\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(gyro_data_tensor), batch_size):\n",
    "            batch = gyro_data_tensor[i:i + batch_size].to(device)\n",
    "            outputs = model(batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predictions.append(predicted.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(all_predictions)\n",
    "\n",
    "# 자이로 데이터에 예측된 라벨 추가\n",
    "original_gyro_data = pd.read_csv('./원본 데이터/자이로 데이터.csv')\n",
    "predictions = predict_labels(best_model, gyro_data_tensor)\n",
    "original_predictions = map_predictions_to_original(len(original_gyro_data), predictions)\n",
    "\n",
    "# 활동 라벨 매핑\n",
    "activity_labels = {0: \"walking\", 1: \"walking_upstairs\", 2: \"walking_downstairs\", 3: \"sitting\", 4: \"standing\", 5: \"laying\"}\n",
    "predicted_activities = [activity_labels[label] for label in original_predictions]\n",
    "\n",
    "# 자이로 데이터에 예측 결과 추가\n",
    "original_gyro_data['Predicted_Activity'] = predicted_activities\n",
    "\n",
    "# 결과를 CSV 파일로 저장\n",
    "original_gyro_data.to_csv('./원본 데이터/자이로 데이터_예측_v2.0.csv', index=False)\n",
    "\n",
    "# 예측 결과 확인 (상위 5개의 샘플만 출력)\n",
    "print(original_gyro_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a663950-c9d3-4044-8dce-1563d771dbbe",
   "metadata": {},
   "source": [
    "# 11. 결과 시각화\n",
    "***\n",
    "> 예측된 활동 라벨의 분포 및 시간에 따른 변화를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7a85b6-44cf-4202-941d-70f58c787930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측된 행동 패턴 분포 시각화\n",
    "plt.figure(figsize=(12, 6))\n",
    "activity_counts = pd.Series(predicted_activities).value_counts()\n",
    "sns.barplot(x=activity_counts.index, y=activity_counts.values)\n",
    "plt.title('Predicted Activity Distribution')\n",
    "plt.xlabel('Activity')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 시간에 따른 활동 변화 시각화\n",
    "plt.figure(figsize=(20, 6))\n",
    "activity_series = pd.Series(predicted_activities)\n",
    "activity_numeric = pd.factorize(activity_series)[0]\n",
    "plt.plot(activity_numeric)\n",
    "plt.title('Activity Changes Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Activity')\n",
    "plt.yticks(range(len(activity_labels)), list(activity_labels.values()))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
