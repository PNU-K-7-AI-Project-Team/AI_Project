{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c61400e5-f292-4fec-9063-ef6baa48a148",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.fft import fft\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.signal import resample\n",
    "from scipy.stats import skew, kurtosis, entropy, uniform\n",
    "from scipy.signal import find_peaks, stft\n",
    "from sklearn.model_selection import KFold, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from skorch import NeuralNetClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8fa626e-4ca4-45fb-99d6-54a4c64d6e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 로드 및 전처리\n",
    "\n",
    "# 자이로 데이터 로드 (RegisterDate를 datetime으로 변환)\n",
    "gyro_data = pd.read_csv('./원본 데이터/자이로 데이터.csv')\n",
    "gyro_data['RegisterDate'] = pd.to_datetime(gyro_data['RegisterDate'])\n",
    "\n",
    "# UCI-HAR 데이터 로드\n",
    "uci_har_path = './원본 데이터/UCI HAR Dataset/'\n",
    "gyro_x_train = pd.read_csv(uci_har_path + 'train/Inertial Signals/body_gyro_x_train.txt', sep='\\s+', header=None).values\n",
    "gyro_y_train = pd.read_csv(uci_har_path + 'train/Inertial Signals/body_gyro_y_train.txt', sep='\\s+', header=None).values\n",
    "gyro_z_train = pd.read_csv(uci_har_path + 'train/Inertial Signals/body_gyro_z_train.txt', sep='\\s+', header=None).values\n",
    "\n",
    "# X, Y, Z 축 데이터를 DataFrame으로 병합\n",
    "uci_har_gyro_df = pd.DataFrame({\n",
    "    'X': np.mean(gyro_x_train, axis=1),\n",
    "    'Y': np.mean(gyro_y_train, axis=1),\n",
    "    'Z': np.mean(gyro_z_train, axis=1)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95e952d5-7270-41cc-855a-6840e0a7b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 주파수 업샘플링 (UCI-HAR 데이터를 50Hz에서 100Hz로 업샘플링)\n",
    "current_freq = 50  # UCI-HAR 데이터 주파수\n",
    "desired_freq = 100  # 자이로 데이터 주파수\n",
    "\n",
    "# 보간법을 사용한 업샘플링\n",
    "t_current = np.linspace(0, len(uci_har_gyro_df) / current_freq, num=len(uci_har_gyro_df))\n",
    "t_new = np.linspace(0, len(uci_har_gyro_df) / current_freq, num=len(uci_har_gyro_df) * 2)\n",
    "\n",
    "# 새로운 DataFrame에 업샘플링된 데이터 저장\n",
    "uci_har_gyro_df_upsampled = pd.DataFrame({\n",
    "    'X': np.interp(t_new, t_current, uci_har_gyro_df['X']),\n",
    "    'Y': np.interp(t_new, t_current, uci_har_gyro_df['Y']),\n",
    "    'Z': np.interp(t_new, t_current, uci_har_gyro_df['Z'])\n",
    "})\n",
    "\n",
    "# 자이로 데이터를 UCI-HAR 데이터의 길이로 슬라이싱\n",
    "n_samples_uci = len(uci_har_gyro_df)\n",
    "gyro_sliced = gyro_data.iloc[:n_samples_uci].copy()  # UCI-HAR 데이터 길이만큼 자이로 데이터를 슬라이싱\n",
    "\n",
    "# 데이터 정규화 (MinMaxScaler로 바로 inplace로 변환)\n",
    "scaler = MinMaxScaler()\n",
    "gyro_sliced[['X', 'Y', 'Z']] = scaler.fit_transform(gyro_sliced[['X', 'Y', 'Z']])\n",
    "uci_har_gyro_df[['X', 'Y', 'Z']] = scaler.fit_transform(uci_har_gyro_df[['X', 'Y', 'Z']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66f30f7a-8d33-4aad-8a9f-bbcad5eab090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_features shape: (14704, 21)\n",
      "y_train_upsampled shape: (14704,)\n"
     ]
    }
   ],
   "source": [
    "# RMS 함수 정의\n",
    "def rms(values):\n",
    "    return np.sqrt(np.mean(values**2))\n",
    "\n",
    "# 엔트로피 계산 함수 정의\n",
    "def calc_entropy(values):\n",
    "    # 확률 밀도 함수 계산 후 엔트로피 계산\n",
    "    value_prob = np.histogram(values, bins=30, density=True)[0]  # 확률 밀도 함수\n",
    "    return entropy(value_prob + 1e-6)  # 엔트로피 계산\n",
    "\n",
    "# FFT 특징 계산 함수\n",
    "def fft_features(values, n=10):\n",
    "    fft_vals = np.abs(fft(values))  # FFT 계산 후 절댓값을 취함\n",
    "    return np.mean(fft_vals[:n])  # 주파수 성분의 상위 N개 평균 계산\n",
    "\n",
    "# STFT 특징 계산 함수\n",
    "def stft_features(values, n=10):\n",
    "    _, _, Zxx = stft(values)\n",
    "    Zxx_flat = np.abs(Zxx).flatten()  # STFT 결과를 1차원으로 변환\n",
    "    Zxx_mean = np.mean(Zxx_flat)  # 플랫한 결과의 평균값을 계산\n",
    "    return Zxx_mean\n",
    "\n",
    "# RMS, Skewness, Kurtosis, Entropy 계산 및 피크 탐지 함수\n",
    "def calculate_features(df, axis):\n",
    "    df[f'rms_{axis}'] = rms(df[axis])\n",
    "    df[f'skew_{axis}'] = skew(df[axis])\n",
    "    df[f'kurtosis_{axis}'] = kurtosis(df[axis])\n",
    "    df[f'entropy_{axis}'] = calc_entropy(df[axis])\n",
    "    \n",
    "    # 피크 탐지\n",
    "    peaks, _ = find_peaks(df[axis], height=0)\n",
    "    df[f'peaks_{axis}'] = 0\n",
    "    df.loc[peaks, f'peaks_{axis}'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 각 축에 대해 특성 계산\n",
    "for axis in ['X', 'Y', 'Z']:\n",
    "    uci_har_gyro_df_upsampled = calculate_features(uci_har_gyro_df_upsampled, axis)\n",
    "    gyro_sliced = calculate_features(gyro_sliced, axis)\n",
    "\n",
    "# 푸리에 변환 (FFT) 및 STFT 계산 함수\n",
    "def calculate_fft_stft(df, axis, n=10):\n",
    "    # FFT 계산\n",
    "    df[f'fft_{axis}'] = fft_features(df[axis].values, n=n)\n",
    "    \n",
    "    # STFT 계산\n",
    "    stft_result = stft_features(df[axis].values)\n",
    "    df[f'stft_{axis}'] = stft_result\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 각 축에 대해 FFT 및 STFT 계산\n",
    "for axis in ['X', 'Y', 'Z']:\n",
    "    uci_har_gyro_df_upsampled = calculate_fft_stft(uci_har_gyro_df_upsampled, axis)\n",
    "    gyro_sliced = calculate_fft_stft(gyro_sliced, axis)\n",
    "\n",
    "# 데이터 정규화 (MinMaxScaler)\n",
    "scaler = MinMaxScaler()\n",
    "# 컬럼 이름의 대소문자를 일치시킵니다.\n",
    "columns_to_scale = ['fft_X', 'fft_Y', 'fft_Z', 'stft_X', 'stft_Y', 'stft_Z']\n",
    "uci_har_gyro_df_upsampled[columns_to_scale] = scaler.fit_transform(uci_har_gyro_df_upsampled[columns_to_scale])\n",
    "gyro_sliced[columns_to_scale] = scaler.fit_transform(gyro_sliced[columns_to_scale])\n",
    "\n",
    "# 최종 피처 세트 구축\n",
    "X_train_features = np.column_stack((\n",
    "    uci_har_gyro_df_upsampled[['X', 'Y', 'Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['rms_X', 'rms_Y', 'rms_Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['skew_X', 'skew_Y', 'skew_Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['entropy_X', 'entropy_Y', 'entropy_Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['fft_X', 'fft_Y', 'fft_Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['peaks_X', 'peaks_Y', 'peaks_Z']].values,\n",
    "    uci_har_gyro_df_upsampled[['stft_X', 'stft_Y', 'stft_Z']].values\n",
    "))\n",
    "\n",
    "X_gyro_features = np.column_stack((\n",
    "    gyro_sliced[['X', 'Y', 'Z']].values,\n",
    "    gyro_sliced[['rms_X', 'rms_Y', 'rms_Z']].values,\n",
    "    gyro_sliced[['skew_X', 'skew_Y', 'skew_Z']].values,\n",
    "    gyro_sliced[['entropy_X', 'entropy_Y', 'entropy_Z']].values,\n",
    "    gyro_sliced[['fft_X', 'fft_Y', 'fft_Z']].values,\n",
    "    gyro_sliced[['peaks_X', 'peaks_Y', 'peaks_Z']].values,\n",
    "    gyro_sliced[['stft_X', 'stft_Y', 'stft_Z']].values\n",
    "))\n",
    "\n",
    "# 라벨 추가 (UCI-HAR 데이터에 라벨링 있음)\n",
    "y_train = pd.read_csv(uci_har_path + 'train/y_train.txt', header=None).values.flatten()\n",
    "\n",
    "# y_train을 X_train_features의 크기에 맞추어 반복 (업샘플링)\n",
    "y_train_upsampled = np.repeat(y_train, 2)  # 레이블을 2배로 확장\n",
    "\n",
    "# 데이터 크기 일치 확인\n",
    "print(f\"X_train_features shape: {X_train_features.shape}\")\n",
    "print(f\"y_train_upsampled shape: {y_train_upsampled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c97ded0-3ed9-4e9f-b1ce-254b2de112b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 학습용과 검증용으로 나누기\n",
    "X_train_features, X_val, y_train_upsampled, y_val = train_test_split(X_train_features, \n",
    "                                                                     y_train_upsampled, \n",
    "                                                                     test_size=0.2, \n",
    "                                                                     random_state=42)\n",
    "\n",
    "# 데이터 차원 확장 (batch_size, sequence_length, input_size) 형식으로 맞추기\n",
    "X_train_features = X_train_features.reshape(X_train_features.shape[0], 1, X_train_features.shape[1])\n",
    "X_val = X_val.reshape(X_val.shape[0], 1, X_val.shape[1])\n",
    "\n",
    "# y_train은 그대로 둡니다. 이 변수는 차원 확장이 필요하지 않습니다.\n",
    "y_train = y_train_upsampled.astype(np.int64)\n",
    "y_val = y_val.astype(np.int64)\n",
    "\n",
    "# 데이터셋 및 데이터로더 정의\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_features, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "val_dataset = TensorDataset(torch.tensor(X_val, dtype=torch.float32), torch.tensor(y_val, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a995c5bb-5f84-44df-afc2-20212fbfa312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 정의 (LSTM, GRU, CNN-LSTM, BiLSTM, Transformer)\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.lstm.num_layers, x.size(0), self.lstm.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.gru.num_layers, x.size(0), self.gru.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.lstm = nn.LSTM(64, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)  # CNN에 맞춰 입력 차원 확장\n",
    "        x = self.conv(x.permute(0, 2, 1))\n",
    "        out, _ = self.lstm(x.permute(0, 2, 1))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.bilstm.num_layers * 2, x.size(0), self.bilstm.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.bilstm.num_layers * 2, x.size(0), self.bilstm.hidden_size).to(x.device)\n",
    "        out, _ = self.bilstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_heads, num_layers, num_classes):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Linear(input_size, hidden_size)\n",
    "        self.transformer = nn.Transformer(d_model=hidden_size, nhead=num_heads, num_encoder_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.transformer(x)\n",
    "        out = self.fc(x[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14b332f-4415-4200-8074-fe83e8a4c475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing model: LSTMModel\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.7012\u001b[0m       \u001b[32m0.3438\u001b[0m        \u001b[35m1.4605\u001b[0m  0.6054\n",
      "      2        \u001b[36m1.4610\u001b[0m       0.3379        \u001b[35m1.4598\u001b[0m  0.5545\n",
      "      3        \u001b[36m1.4249\u001b[0m       0.3294        \u001b[35m1.4317\u001b[0m  0.5565\n",
      "      4        \u001b[36m1.4173\u001b[0m       \u001b[32m0.3512\u001b[0m        \u001b[35m1.3877\u001b[0m  0.5575\n",
      "      5        \u001b[36m1.3935\u001b[0m       \u001b[32m0.3634\u001b[0m        \u001b[35m1.3852\u001b[0m  0.5565\n",
      "      6        \u001b[36m1.3594\u001b[0m       \u001b[32m0.3858\u001b[0m        \u001b[35m1.3443\u001b[0m  0.5585\n",
      "      7        \u001b[36m1.3412\u001b[0m       \u001b[32m0.3996\u001b[0m        1.3595  0.5555\n",
      "      8        \u001b[36m1.3202\u001b[0m       0.3889        1.4124  0.5565\n",
      "      9        \u001b[36m1.2975\u001b[0m       \u001b[32m0.4230\u001b[0m        \u001b[35m1.2715\u001b[0m  0.5545\n",
      "     10        \u001b[36m1.2837\u001b[0m       \u001b[32m0.4299\u001b[0m        \u001b[35m1.2548\u001b[0m  0.5555\n",
      "     11        \u001b[36m1.2742\u001b[0m       \u001b[32m0.4378\u001b[0m        \u001b[35m1.2478\u001b[0m  0.5605\n",
      "     12        \u001b[36m1.2618\u001b[0m       \u001b[32m0.4405\u001b[0m        \u001b[35m1.2432\u001b[0m  0.5785\n",
      "     13        \u001b[36m1.2507\u001b[0m       \u001b[32m0.4469\u001b[0m        1.2455  0.5854\n",
      "     14        \u001b[36m1.2367\u001b[0m       0.4447        1.2556  0.5974\n",
      "     15        1.2408       \u001b[32m0.4586\u001b[0m        \u001b[35m1.2268\u001b[0m  0.5974\n",
      "     16        1.2386       0.4564        1.2277  0.6024\n",
      "     17        1.2437       0.4516        1.2289  0.6064\n",
      "     18        \u001b[36m1.2231\u001b[0m       0.4490        1.2525  0.6064\n",
      "     19        1.2285       0.4564        \u001b[35m1.2107\u001b[0m  0.6094\n",
      "     20        \u001b[36m1.2096\u001b[0m       0.4400        1.2479  0.6094\n",
      "[CV] END batch_size=64, lr=0.008065429868602328, module__hidden_size=256, module__num_layers=3; total time=  12.4s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.7603\u001b[0m       \u001b[32m0.3177\u001b[0m        \u001b[35m1.5933\u001b[0m  0.6154\n",
      "      2        \u001b[36m1.4676\u001b[0m       \u001b[32m0.3486\u001b[0m        \u001b[35m1.4331\u001b[0m  0.6193\n",
      "      3        \u001b[36m1.4330\u001b[0m       0.3464        1.4414  0.6183\n",
      "      4        \u001b[36m1.4130\u001b[0m       0.3385        \u001b[35m1.4016\u001b[0m  0.6233\n",
      "      5        \u001b[36m1.3922\u001b[0m       \u001b[32m0.3549\u001b[0m        1.4268  0.6233\n",
      "      6        1.3973       \u001b[32m0.3735\u001b[0m        \u001b[35m1.3763\u001b[0m  0.6273\n",
      "      7        \u001b[36m1.3812\u001b[0m       0.3475        1.3971  0.6283\n",
      "      8        \u001b[36m1.3629\u001b[0m       \u001b[32m0.3783\u001b[0m        \u001b[35m1.3552\u001b[0m  0.6303\n",
      "      9        \u001b[36m1.3324\u001b[0m       \u001b[32m0.3836\u001b[0m        \u001b[35m1.3109\u001b[0m  0.6263\n",
      "     10        \u001b[36m1.2950\u001b[0m       \u001b[32m0.4357\u001b[0m        \u001b[35m1.2815\u001b[0m  0.6273\n",
      "     11        1.2962       0.4309        \u001b[35m1.2812\u001b[0m  0.6243\n",
      "     12        \u001b[36m1.2779\u001b[0m       0.4086        \u001b[35m1.2790\u001b[0m  0.6263\n",
      "     13        \u001b[36m1.2559\u001b[0m       \u001b[32m0.4548\u001b[0m        \u001b[35m1.2539\u001b[0m  0.6253\n",
      "     14        \u001b[36m1.2528\u001b[0m       0.4405        1.2666  0.6263\n",
      "     15        \u001b[36m1.2490\u001b[0m       0.4447        1.2622  0.6263\n",
      "     16        \u001b[36m1.2439\u001b[0m       \u001b[32m0.4554\u001b[0m        \u001b[35m1.2327\u001b[0m  0.6263\n",
      "     17        \u001b[36m1.2350\u001b[0m       0.4410        1.2571  0.6233\n",
      "     18        \u001b[36m1.2226\u001b[0m       0.4416        1.2440  0.6233\n",
      "     19        1.2234       0.4495        \u001b[35m1.2313\u001b[0m  0.6233\n",
      "     20        \u001b[36m1.2160\u001b[0m       \u001b[32m0.4580\u001b[0m        \u001b[35m1.2289\u001b[0m  0.6283\n",
      "[CV] END batch_size=64, lr=0.008065429868602328, module__hidden_size=256, module__num_layers=3; total time=  12.6s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.6184\u001b[0m       \u001b[32m0.2912\u001b[0m        \u001b[35m1.5195\u001b[0m  0.6313\n",
      "      2        \u001b[36m1.4412\u001b[0m       \u001b[32m0.3411\u001b[0m        \u001b[35m1.4108\u001b[0m  0.6263\n",
      "      3        \u001b[36m1.4123\u001b[0m       \u001b[32m0.3597\u001b[0m        1.4119  0.6313\n",
      "      4        \u001b[36m1.4030\u001b[0m       \u001b[32m0.3704\u001b[0m        \u001b[35m1.3780\u001b[0m  0.6363\n",
      "      5        \u001b[36m1.3444\u001b[0m       \u001b[32m0.3847\u001b[0m        \u001b[35m1.3557\u001b[0m  0.6293\n",
      "      6        \u001b[36m1.3045\u001b[0m       0.3714        \u001b[35m1.3372\u001b[0m  0.6283\n",
      "      7        \u001b[36m1.2947\u001b[0m       \u001b[32m0.4091\u001b[0m        \u001b[35m1.3115\u001b[0m  0.6263\n",
      "      8        \u001b[36m1.2886\u001b[0m       \u001b[32m0.4368\u001b[0m        \u001b[35m1.2664\u001b[0m  0.6313\n",
      "      9        \u001b[36m1.2797\u001b[0m       0.4320        1.2723  0.6283\n",
      "     10        \u001b[36m1.2619\u001b[0m       \u001b[32m0.4410\u001b[0m        1.2721  0.6263\n",
      "     11        \u001b[36m1.2618\u001b[0m       0.4389        \u001b[35m1.2600\u001b[0m  0.6263\n",
      "     12        \u001b[36m1.2496\u001b[0m       0.4352        \u001b[35m1.2551\u001b[0m  0.6263\n",
      "     13        \u001b[36m1.2476\u001b[0m       \u001b[32m0.4740\u001b[0m        \u001b[35m1.2208\u001b[0m  0.6293\n",
      "     14        \u001b[36m1.2321\u001b[0m       0.4575        1.2286  0.6273\n",
      "     15        1.2357       0.4421        1.2555  0.6253\n",
      "     16        \u001b[36m1.2179\u001b[0m       0.4623        1.2284  0.6263\n",
      "     17        1.2189       0.4516        1.2286  0.6273\n",
      "     18        \u001b[36m1.2156\u001b[0m       0.4368        1.2367  0.6263\n",
      "     19        \u001b[36m1.2033\u001b[0m       0.4474        \u001b[35m1.2094\u001b[0m  0.6283\n",
      "     20        \u001b[36m1.1987\u001b[0m       0.4532        1.2146  0.6303\n",
      "[CV] END batch_size=64, lr=0.008065429868602328, module__hidden_size=256, module__num_layers=3; total time=  12.7s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.7825\u001b[0m       \u001b[32m0.2273\u001b[0m        \u001b[35m1.7089\u001b[0m  0.6283\n",
      "      2        \u001b[36m1.5184\u001b[0m       \u001b[32m0.3287\u001b[0m        \u001b[35m1.4428\u001b[0m  0.6303\n",
      "      3        \u001b[36m1.4581\u001b[0m       \u001b[32m0.3383\u001b[0m        1.4716  0.6323\n",
      "      4        \u001b[36m1.4119\u001b[0m       \u001b[32m0.3463\u001b[0m        \u001b[35m1.4181\u001b[0m  0.6313\n",
      "      5        \u001b[36m1.3774\u001b[0m       \u001b[32m0.3733\u001b[0m        \u001b[35m1.3613\u001b[0m  0.6323\n",
      "      6        \u001b[36m1.3651\u001b[0m       \u001b[32m0.3941\u001b[0m        \u001b[35m1.3305\u001b[0m  0.6313\n",
      "      7        \u001b[36m1.3224\u001b[0m       \u001b[32m0.4509\u001b[0m        \u001b[35m1.2698\u001b[0m  0.6293\n",
      "      8        \u001b[36m1.2968\u001b[0m       0.4264        \u001b[35m1.2688\u001b[0m  0.6323\n",
      "      9        \u001b[36m1.2789\u001b[0m       0.4121        1.3000  0.6293\n",
      "     10        \u001b[36m1.2638\u001b[0m       0.4286        \u001b[35m1.2569\u001b[0m  0.6333\n",
      "     11        1.2692       0.4121        1.2690  0.6523\n",
      "     12        \u001b[36m1.2448\u001b[0m       0.4211        1.2594  0.6602\n",
      "     13        1.2478       0.4413        \u001b[35m1.2528\u001b[0m  0.6473\n",
      "     14        \u001b[36m1.2303\u001b[0m       \u001b[32m0.4514\u001b[0m        \u001b[35m1.2523\u001b[0m  0.6353\n",
      "     15        1.2323       0.4445        1.2662  0.6333\n",
      "     16        1.2328       \u001b[32m0.4557\u001b[0m        \u001b[35m1.2281\u001b[0m  0.6303\n",
      "     17        \u001b[36m1.2197\u001b[0m       \u001b[32m0.4588\u001b[0m        1.2360  0.6353\n",
      "     18        \u001b[36m1.2193\u001b[0m       0.4413        1.2354  0.6343\n",
      "     19        \u001b[36m1.2116\u001b[0m       \u001b[32m0.4594\u001b[0m        \u001b[35m1.2156\u001b[0m  0.6323\n",
      "     20        \u001b[36m1.2029\u001b[0m       0.4588        1.2585  0.6303\n",
      "[CV] END batch_size=64, lr=0.008065429868602328, module__hidden_size=256, module__num_layers=3; total time=  12.8s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.6893\u001b[0m       \u001b[32m0.3144\u001b[0m        \u001b[35m1.4819\u001b[0m  0.6323\n",
      "      2        \u001b[36m1.4575\u001b[0m       \u001b[32m0.3314\u001b[0m        1.5084  0.6303\n",
      "      3        \u001b[36m1.3944\u001b[0m       \u001b[32m0.3813\u001b[0m        \u001b[35m1.3565\u001b[0m  0.6303\n",
      "      4        \u001b[36m1.3455\u001b[0m       \u001b[32m0.4063\u001b[0m        \u001b[35m1.3068\u001b[0m  0.9864\n",
      "      5        \u001b[36m1.3113\u001b[0m       \u001b[32m0.4233\u001b[0m        \u001b[35m1.2923\u001b[0m  0.6343\n",
      "      6        \u001b[36m1.2889\u001b[0m       0.4079        \u001b[35m1.2705\u001b[0m  0.6363\n",
      "      7        \u001b[36m1.2748\u001b[0m       0.4121        1.3311  0.6323\n",
      "      8        1.2825       \u001b[32m0.4275\u001b[0m        1.2746  0.6263\n",
      "      9        \u001b[36m1.2608\u001b[0m       \u001b[32m0.4381\u001b[0m        \u001b[35m1.2639\u001b[0m  0.6253\n",
      "     10        \u001b[36m1.2586\u001b[0m       \u001b[32m0.4509\u001b[0m        \u001b[35m1.2602\u001b[0m  0.6303\n",
      "     11        \u001b[36m1.2562\u001b[0m       0.4403        \u001b[35m1.2416\u001b[0m  0.6303\n",
      "     12        \u001b[36m1.2408\u001b[0m       0.4148        1.2902  0.6293\n",
      "     13        \u001b[36m1.2344\u001b[0m       \u001b[32m0.4535\u001b[0m        \u001b[35m1.2349\u001b[0m  0.6323\n",
      "     14        \u001b[36m1.2262\u001b[0m       0.4514        1.2685  0.6363\n",
      "     15        \u001b[36m1.2214\u001b[0m       \u001b[32m0.4578\u001b[0m        1.2408  0.6303\n",
      "     16        \u001b[36m1.2105\u001b[0m       0.4323        1.3020  0.6293\n",
      "     17        \u001b[36m1.2079\u001b[0m       0.4429        1.2439  0.6293\n",
      "     18        \u001b[36m1.2062\u001b[0m       \u001b[32m0.4769\u001b[0m        \u001b[35m1.2133\u001b[0m  0.6283\n",
      "     19        \u001b[36m1.1971\u001b[0m       0.4519        1.2319  0.6283\n",
      "     20        \u001b[36m1.1826\u001b[0m       0.4424        1.2298  0.6263\n",
      "[CV] END batch_size=64, lr=0.008065429868602328, module__hidden_size=256, module__num_layers=3; total time=  13.1s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.6717\u001b[0m       \u001b[32m0.3103\u001b[0m        \u001b[35m1.4864\u001b[0m  2.0980\n",
      "      2        \u001b[36m1.4511\u001b[0m       \u001b[32m0.3603\u001b[0m        \u001b[35m1.4413\u001b[0m  1.9715\n",
      "      3        \u001b[36m1.4154\u001b[0m       0.3252        1.4803  2.0594\n",
      "      4        \u001b[36m1.4055\u001b[0m       0.3385        \u001b[35m1.4368\u001b[0m  1.9503\n",
      "      5        \u001b[36m1.3722\u001b[0m       \u001b[32m0.3911\u001b[0m        \u001b[35m1.3189\u001b[0m  1.9332\n",
      "      6        \u001b[36m1.3472\u001b[0m       0.3773        1.3270  1.8678\n",
      "      7        \u001b[36m1.3187\u001b[0m       \u001b[32m0.3996\u001b[0m        \u001b[35m1.2974\u001b[0m  1.8926\n",
      "      8        \u001b[36m1.3013\u001b[0m       0.3985        1.3164  1.9232\n",
      "      9        \u001b[36m1.2846\u001b[0m       \u001b[32m0.4054\u001b[0m        1.3054  1.8806\n",
      "     10        \u001b[36m1.2737\u001b[0m       \u001b[32m0.4230\u001b[0m        \u001b[35m1.2515\u001b[0m  1.9379\n",
      "     11        \u001b[36m1.2625\u001b[0m       \u001b[32m0.4277\u001b[0m        \u001b[35m1.2386\u001b[0m  1.9024\n",
      "     12        \u001b[36m1.2573\u001b[0m       \u001b[32m0.4543\u001b[0m        \u001b[35m1.2264\u001b[0m  1.9597\n",
      "     13        \u001b[36m1.2442\u001b[0m       0.4410        1.2439  1.9484\n",
      "     14        \u001b[36m1.2374\u001b[0m       0.4431        1.2268  1.9505\n",
      "     15        1.2404       \u001b[32m0.4575\u001b[0m        \u001b[35m1.2205\u001b[0m  1.8868\n",
      "     16        1.2383       \u001b[32m0.4617\u001b[0m        1.2364  1.9094\n",
      "     17        \u001b[36m1.2264\u001b[0m       0.4373        1.2672  1.8804\n",
      "     18        \u001b[36m1.2244\u001b[0m       \u001b[32m0.4729\u001b[0m        \u001b[35m1.2135\u001b[0m  1.9479\n",
      "     19        \u001b[36m1.2109\u001b[0m       0.4511        1.2267  1.9476\n",
      "     20        1.2162       0.4543        1.2141  1.9201\n",
      "[CV] END batch_size=16, lr=0.006068501579464871, module__hidden_size=128, module__num_layers=3; total time=  39.2s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.6692\u001b[0m       \u001b[32m0.3119\u001b[0m        \u001b[35m1.4597\u001b[0m  1.8276\n",
      "      2        \u001b[36m1.4444\u001b[0m       \u001b[32m0.3518\u001b[0m        \u001b[35m1.4111\u001b[0m  1.8787\n",
      "      3        \u001b[36m1.4117\u001b[0m       0.3438        \u001b[35m1.3774\u001b[0m  1.9350\n",
      "      4        \u001b[36m1.3891\u001b[0m       \u001b[32m0.3624\u001b[0m        \u001b[35m1.3566\u001b[0m  1.9045\n",
      "      5        \u001b[36m1.3312\u001b[0m       \u001b[32m0.4022\u001b[0m        \u001b[35m1.3330\u001b[0m  1.9175\n",
      "      6        \u001b[36m1.3190\u001b[0m       0.3927        1.3336  1.9544\n",
      "      7        \u001b[36m1.2986\u001b[0m       0.3868        1.3404  1.9311\n",
      "      8        \u001b[36m1.2840\u001b[0m       \u001b[32m0.4086\u001b[0m        \u001b[35m1.2971\u001b[0m  1.8984\n",
      "      9        \u001b[36m1.2767\u001b[0m       \u001b[32m0.4362\u001b[0m        \u001b[35m1.2795\u001b[0m  1.9518\n",
      "     10        \u001b[36m1.2764\u001b[0m       0.4261        \u001b[35m1.2734\u001b[0m  1.9473\n",
      "     11        \u001b[36m1.2634\u001b[0m       0.4336        \u001b[35m1.2623\u001b[0m  1.8936\n",
      "     12        \u001b[36m1.2529\u001b[0m       0.4315        \u001b[35m1.2498\u001b[0m  1.9441\n",
      "     13        \u001b[36m1.2509\u001b[0m       \u001b[32m0.4426\u001b[0m        \u001b[35m1.2435\u001b[0m  1.9167\n",
      "     14        \u001b[36m1.2477\u001b[0m       0.4134        1.2928  1.8903\n",
      "     15        \u001b[36m1.2441\u001b[0m       0.4113        1.3188  1.9334\n",
      "     16        1.2488       \u001b[32m0.4633\u001b[0m        1.2592  1.9104\n",
      "     17        \u001b[36m1.2331\u001b[0m       0.4437        1.2457  1.9488\n",
      "     18        1.2359       0.4458        \u001b[35m1.2238\u001b[0m  1.9633\n",
      "     19        \u001b[36m1.2262\u001b[0m       0.4511        1.2387  1.9530\n",
      "     20        \u001b[36m1.2206\u001b[0m       \u001b[32m0.4649\u001b[0m        1.2282  2.1211\n",
      "[CV] END batch_size=16, lr=0.006068501579464871, module__hidden_size=128, module__num_layers=3; total time=  39.0s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.7641\u001b[0m       \u001b[32m0.2901\u001b[0m        \u001b[35m1.5920\u001b[0m  1.7913\n",
      "      2        \u001b[36m1.4623\u001b[0m       \u001b[32m0.3337\u001b[0m        \u001b[35m1.4250\u001b[0m  1.8572\n",
      "      3        \u001b[36m1.4204\u001b[0m       \u001b[32m0.3470\u001b[0m        1.4468  1.9131\n",
      "      4        \u001b[36m1.3984\u001b[0m       \u001b[32m0.3565\u001b[0m        \u001b[35m1.3747\u001b[0m  1.8773\n",
      "      5        \u001b[36m1.3606\u001b[0m       \u001b[32m0.3874\u001b[0m        \u001b[35m1.3345\u001b[0m  1.9370\n",
      "      6        \u001b[36m1.3391\u001b[0m       \u001b[32m0.4145\u001b[0m        \u001b[35m1.3098\u001b[0m  1.8768\n",
      "      7        \u001b[36m1.3183\u001b[0m       0.3943        1.3200  1.9463\n",
      "      8        \u001b[36m1.3141\u001b[0m       0.4012        \u001b[35m1.3039\u001b[0m  1.9261\n",
      "      9        \u001b[36m1.2956\u001b[0m       0.3836        1.3760  1.9087\n",
      "     10        \u001b[36m1.2720\u001b[0m       \u001b[32m0.4426\u001b[0m        \u001b[35m1.2429\u001b[0m  1.9568\n",
      "     11        \u001b[36m1.2667\u001b[0m       0.4346        1.2717  1.9478\n",
      "     12        \u001b[36m1.2531\u001b[0m       \u001b[32m0.4490\u001b[0m        1.2482  1.9517\n",
      "     13        \u001b[36m1.2526\u001b[0m       0.4283        1.2650  1.9392\n",
      "     14        \u001b[36m1.2452\u001b[0m       0.4479        1.2653  1.9479\n",
      "     15        1.2459       0.4267        1.2495  1.9400\n",
      "     16        \u001b[36m1.2298\u001b[0m       \u001b[32m0.4506\u001b[0m        \u001b[35m1.2317\u001b[0m  1.9130\n",
      "     17        1.2301       0.4421        1.2623  2.0067\n",
      "     18        1.2326       \u001b[32m0.4580\u001b[0m        1.2389  1.9472\n",
      "     19        \u001b[36m1.2255\u001b[0m       \u001b[32m0.4766\u001b[0m        \u001b[35m1.2197\u001b[0m  1.9774\n",
      "     20        \u001b[36m1.2175\u001b[0m       0.4501        1.2464  1.9708\n",
      "[CV] END batch_size=16, lr=0.006068501579464871, module__hidden_size=128, module__num_layers=3; total time=  39.0s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.7865\u001b[0m       \u001b[32m0.1917\u001b[0m        \u001b[35m1.7836\u001b[0m  1.8649\n",
      "      2        \u001b[36m1.6218\u001b[0m       \u001b[32m0.3319\u001b[0m        \u001b[35m1.4376\u001b[0m  1.8525\n",
      "      3        \u001b[36m1.4126\u001b[0m       \u001b[32m0.3409\u001b[0m        1.4416  1.8816\n",
      "      4        \u001b[36m1.3884\u001b[0m       \u001b[32m0.3505\u001b[0m        \u001b[35m1.3799\u001b[0m  1.8861\n",
      "      5        \u001b[36m1.3661\u001b[0m       \u001b[32m0.3956\u001b[0m        \u001b[35m1.3322\u001b[0m  1.8745\n",
      "      6        \u001b[36m1.3156\u001b[0m       0.3941        \u001b[35m1.3054\u001b[0m  1.9345\n",
      "      7        \u001b[36m1.2930\u001b[0m       \u001b[32m0.4142\u001b[0m        \u001b[35m1.2720\u001b[0m  1.9292\n",
      "      8        \u001b[36m1.2831\u001b[0m       \u001b[32m0.4365\u001b[0m        \u001b[35m1.2627\u001b[0m  1.8946\n",
      "      9        \u001b[36m1.2636\u001b[0m       0.3962        1.3432  1.9328\n",
      "     10        \u001b[36m1.2581\u001b[0m       0.4249        \u001b[35m1.2551\u001b[0m  1.8879\n",
      "     11        \u001b[36m1.2407\u001b[0m       0.4318        \u001b[35m1.2427\u001b[0m  1.9525\n",
      "     12        1.2411       \u001b[32m0.4408\u001b[0m        \u001b[35m1.2424\u001b[0m  1.9510\n",
      "     13        \u001b[36m1.2351\u001b[0m       \u001b[32m0.4562\u001b[0m        1.2523  1.9375\n",
      "     14        \u001b[36m1.2253\u001b[0m       0.4472        \u001b[35m1.2383\u001b[0m  1.9009\n",
      "     15        \u001b[36m1.2191\u001b[0m       0.4355        1.2485  1.9528\n",
      "     16        1.2218       0.4403        1.2879  1.9362\n",
      "     17        \u001b[36m1.2128\u001b[0m       0.4488        1.2472  1.8969\n",
      "     18        \u001b[36m1.2075\u001b[0m       0.4456        \u001b[35m1.2276\u001b[0m  1.9609\n",
      "     19        1.2097       0.4525        1.2448  1.9649\n",
      "     20        1.2087       \u001b[32m0.4567\u001b[0m        \u001b[35m1.2260\u001b[0m  1.9499\n",
      "[CV] END batch_size=16, lr=0.006068501579464871, module__hidden_size=128, module__num_layers=3; total time=  38.8s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.6015\u001b[0m       \u001b[32m0.3314\u001b[0m        \u001b[35m1.4609\u001b[0m  1.8369\n",
      "      2        \u001b[36m1.4343\u001b[0m       \u001b[32m0.3356\u001b[0m        \u001b[35m1.4344\u001b[0m  2.0951\n",
      "      3        \u001b[36m1.4106\u001b[0m       \u001b[32m0.3436\u001b[0m        1.4382  1.8979\n",
      "      4        \u001b[36m1.4032\u001b[0m       \u001b[32m0.3670\u001b[0m        \u001b[35m1.3761\u001b[0m  1.9400\n",
      "      5        \u001b[36m1.3815\u001b[0m       \u001b[32m0.3866\u001b[0m        \u001b[35m1.3489\u001b[0m  1.9648\n",
      "      6        \u001b[36m1.3247\u001b[0m       \u001b[32m0.4057\u001b[0m        \u001b[35m1.3096\u001b[0m  1.8958\n",
      "      7        \u001b[36m1.2963\u001b[0m       \u001b[32m0.4180\u001b[0m        \u001b[35m1.2898\u001b[0m  1.9569\n",
      "      8        \u001b[36m1.2909\u001b[0m       0.4036        \u001b[35m1.2880\u001b[0m  1.9620\n",
      "      9        \u001b[36m1.2664\u001b[0m       \u001b[32m0.4259\u001b[0m        \u001b[35m1.2729\u001b[0m  1.9233\n",
      "     10        \u001b[36m1.2606\u001b[0m       0.4243        \u001b[35m1.2688\u001b[0m  1.8745\n",
      "     11        \u001b[36m1.2509\u001b[0m       \u001b[32m0.4450\u001b[0m        \u001b[35m1.2467\u001b[0m  1.9259\n",
      "     12        \u001b[36m1.2434\u001b[0m       0.4206        1.2948  1.8654\n",
      "     13        \u001b[36m1.2421\u001b[0m       \u001b[32m0.4493\u001b[0m        1.2671  1.9283\n",
      "     14        \u001b[36m1.2352\u001b[0m       0.4450        \u001b[35m1.2313\u001b[0m  1.8976\n",
      "     15        \u001b[36m1.2296\u001b[0m       0.4371        1.2786  1.9623\n",
      "     16        \u001b[36m1.2170\u001b[0m       0.4392        1.2530  1.9579\n",
      "     17        1.2226       \u001b[32m0.4535\u001b[0m        \u001b[35m1.2293\u001b[0m  1.9514\n",
      "     18        \u001b[36m1.2060\u001b[0m       0.4344        1.2731  1.9279\n",
      "     19        \u001b[36m1.1959\u001b[0m       0.4477        1.2425  1.9291\n",
      "     20        \u001b[36m1.1930\u001b[0m       0.4397        1.2899  1.9264\n",
      "[CV] END batch_size=16, lr=0.006068501579464871, module__hidden_size=128, module__num_layers=3; total time=  39.0s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.7851\u001b[0m       \u001b[32m0.1977\u001b[0m        \u001b[35m1.7820\u001b[0m  0.4668\n",
      "      2        \u001b[36m1.7825\u001b[0m       0.1977        \u001b[35m1.7819\u001b[0m  0.4658\n",
      "      3        \u001b[36m1.7823\u001b[0m       0.1977        \u001b[35m1.7809\u001b[0m  0.4652\n",
      "      4        \u001b[36m1.7780\u001b[0m       \u001b[32m0.2120\u001b[0m        \u001b[35m1.7686\u001b[0m  0.4823\n",
      "      5        \u001b[36m1.6416\u001b[0m       \u001b[32m0.3236\u001b[0m        \u001b[35m1.4656\u001b[0m  0.5279\n",
      "      6        \u001b[36m1.4388\u001b[0m       \u001b[32m0.3427\u001b[0m        \u001b[35m1.4019\u001b[0m  0.5300\n",
      "      7        \u001b[36m1.4021\u001b[0m       \u001b[32m0.3528\u001b[0m        \u001b[35m1.3863\u001b[0m  0.5364\n",
      "      8        \u001b[36m1.3960\u001b[0m       0.3518        \u001b[35m1.3815\u001b[0m  0.5398\n",
      "      9        \u001b[36m1.3823\u001b[0m       \u001b[32m0.3533\u001b[0m        \u001b[35m1.3728\u001b[0m  0.5451\n",
      "     10        \u001b[36m1.3777\u001b[0m       \u001b[32m0.3698\u001b[0m        \u001b[35m1.3670\u001b[0m  0.5466\n",
      "     11        \u001b[36m1.3709\u001b[0m       0.3560        1.3693  0.5476\n",
      "     12        \u001b[36m1.3671\u001b[0m       0.3645        \u001b[35m1.3612\u001b[0m  0.5583\n",
      "     13        \u001b[36m1.3637\u001b[0m       0.3672        1.3632  0.5582\n",
      "     14        \u001b[36m1.3613\u001b[0m       \u001b[32m0.3735\u001b[0m        1.3661  0.5598\n",
      "     15        \u001b[36m1.3551\u001b[0m       0.3528        \u001b[35m1.3599\u001b[0m  0.5700\n",
      "     16        1.3560       0.3704        \u001b[35m1.3445\u001b[0m  0.5469\n",
      "     17        \u001b[36m1.3471\u001b[0m       0.3719        \u001b[35m1.3396\u001b[0m  0.5464\n",
      "     18        \u001b[36m1.3375\u001b[0m       0.3714        \u001b[35m1.3348\u001b[0m  0.5465\n",
      "     19        \u001b[36m1.3329\u001b[0m       \u001b[32m0.3804\u001b[0m        \u001b[35m1.3308\u001b[0m  0.5505\n",
      "     20        1.3409       \u001b[32m0.3948\u001b[0m        \u001b[35m1.3188\u001b[0m  0.5578\n",
      "[CV] END batch_size=64, lr=0.0006808361216819947, module__hidden_size=64, module__num_layers=3; total time=  10.8s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.7889\u001b[0m       \u001b[32m0.1918\u001b[0m        \u001b[35m1.7844\u001b[0m  0.5795\n",
      "      2        \u001b[36m1.7846\u001b[0m       0.1918        \u001b[35m1.7838\u001b[0m  0.5444\n",
      "      3        \u001b[36m1.7839\u001b[0m       0.1918        1.7840  0.5487\n",
      "      4        \u001b[36m1.7822\u001b[0m       \u001b[32m0.1961\u001b[0m        \u001b[35m1.7769\u001b[0m  0.5721\n",
      "      5        \u001b[36m1.7096\u001b[0m       \u001b[32m0.3066\u001b[0m        \u001b[35m1.5556\u001b[0m  0.5605\n",
      "      6        \u001b[36m1.4607\u001b[0m       \u001b[32m0.3321\u001b[0m        \u001b[35m1.4217\u001b[0m  0.6235\n",
      "      7        \u001b[36m1.4204\u001b[0m       \u001b[32m0.3512\u001b[0m        \u001b[35m1.4098\u001b[0m  0.5804\n",
      "      8        \u001b[36m1.4066\u001b[0m       \u001b[32m0.3544\u001b[0m        \u001b[35m1.3956\u001b[0m  0.5395\n",
      "      9        \u001b[36m1.3934\u001b[0m       \u001b[32m0.3565\u001b[0m        \u001b[35m1.3951\u001b[0m  0.5470\n",
      "     10        \u001b[36m1.3845\u001b[0m       \u001b[32m0.3624\u001b[0m        \u001b[35m1.3878\u001b[0m  0.5475\n",
      "     11        \u001b[36m1.3787\u001b[0m       0.3576        \u001b[35m1.3826\u001b[0m  0.5558\n",
      "     12        \u001b[36m1.3785\u001b[0m       0.3544        1.3856  0.5546\n",
      "     13        \u001b[36m1.3730\u001b[0m       \u001b[32m0.3751\u001b[0m        \u001b[35m1.3757\u001b[0m  0.5595\n",
      "     14        \u001b[36m1.3702\u001b[0m       0.3491        1.3865  0.5508\n",
      "     15        \u001b[36m1.3684\u001b[0m       \u001b[32m0.3783\u001b[0m        1.3807  0.5671\n",
      "     16        \u001b[36m1.3633\u001b[0m       0.3549        1.3810  0.5755\n",
      "     17        \u001b[36m1.3629\u001b[0m       0.3528        1.3874  0.5641\n",
      "     18        \u001b[36m1.3597\u001b[0m       \u001b[32m0.3794\u001b[0m        \u001b[35m1.3674\u001b[0m  0.5873\n",
      "     19        \u001b[36m1.3575\u001b[0m       0.3693        1.3685  0.6082\n",
      "     20        \u001b[36m1.3566\u001b[0m       0.3666        1.3789  0.6133\n",
      "[CV] END batch_size=64, lr=0.0006808361216819947, module__hidden_size=64, module__num_layers=3; total time=  11.5s\n",
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m1.7858\u001b[0m       \u001b[32m0.1929\u001b[0m        \u001b[35m1.7835\u001b[0m  0.6188\n",
      "      2        \u001b[36m1.7836\u001b[0m       0.1929        \u001b[35m1.7832\u001b[0m  0.6080\n",
      "      3        \u001b[36m1.7827\u001b[0m       \u001b[32m0.1939\u001b[0m        \u001b[35m1.7814\u001b[0m  0.5775\n",
      "      4        \u001b[36m1.7369\u001b[0m       \u001b[32m0.2954\u001b[0m        \u001b[35m1.5696\u001b[0m  0.5850\n",
      "      5        \u001b[36m1.4753\u001b[0m       \u001b[32m0.3443\u001b[0m        \u001b[35m1.4301\u001b[0m  0.5793\n"
     ]
    }
   ],
   "source": [
    "# num_classes 정의\n",
    "num_classes = len(np.unique(y_train))  # y_train 데이터에서 고유한 클래스 수 계산\n",
    "\n",
    "# 레이블 값을 1씩 빼서 0부터 시작하도록 변환\n",
    "y_train = y_train - 1\n",
    "y_val = y_val - 1\n",
    "\n",
    "# 모델 리스트 정의\n",
    "models = [\n",
    "    LSTMModel(input_size=X_train_features.shape[2], hidden_size=128, num_layers=2, num_classes=num_classes),\n",
    "    GRUModel(input_size=X_train_features.shape[2], hidden_size=128, num_layers=2, num_classes=num_classes),\n",
    "    CNNLSTMModel(input_size=X_train_features.shape[2], hidden_size=128, num_layers=2, num_classes=num_classes),\n",
    "    BiLSTMModel(input_size=X_train_features.shape[2], hidden_size=128, num_layers=2, num_classes=num_classes),\n",
    "    TransformerModel(input_size=X_train_features.shape[2], hidden_size=128, num_heads=4, num_layers=2, num_classes=num_classes)\n",
    "]\n",
    "\n",
    "# RandomizedSearchCV 하이퍼파라미터 최적화\n",
    "params = {\n",
    "    'lr': uniform(0.0001, 0.01),\n",
    "    'module__hidden_size': [64, 128, 256],\n",
    "    'module__num_layers': [1, 2, 3],\n",
    "    'batch_size': [16, 32, 64]\n",
    "}\n",
    "\n",
    "# 결과 저장\n",
    "best_params_per_model = {}\n",
    "best_scores_per_model = {}\n",
    "\n",
    "# 각 모델에 대해 RandomizedSearchCV 실행\n",
    "for model in models:\n",
    "    print(f\"Optimizing model: {model.__class__.__name__}\")\n",
    "\n",
    "    # NeuralNetClassifier로 모델 래핑\n",
    "    net = NeuralNetClassifier(\n",
    "        module=model,\n",
    "        module__input_size=X_train_features.shape[2],\n",
    "        module__num_classes=num_classes,\n",
    "        criterion=nn.CrossEntropyLoss,\n",
    "        optimizer=optim.Adam,\n",
    "        max_epochs=20,\n",
    "        iterator_train__shuffle=True,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu',\n",
    "        verbose=1  # 매 에포크마다 로그 출력\n",
    "    )\n",
    "\n",
    "    # RandomizedSearchCV 설정\n",
    "    rs = RandomizedSearchCV(\n",
    "        net,\n",
    "        params,\n",
    "        refit=True,\n",
    "        cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        scoring='accuracy',\n",
    "        n_iter=10,\n",
    "        verbose=2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # 모델 최적화\n",
    "    X_train_np = X_train_features.astype(np.float32)  # numpy 형식으로 변환\n",
    "    y_train_np = y_train.astype(np.int64)\n",
    "    \n",
    "    rs.fit(X_train_np, y_train_np)  # numpy 데이터 사용\n",
    "\n",
    "    # 최적 하이퍼파라미터와 점수 저장\n",
    "    best_params_per_model[model.__class__.__name__] = rs.best_params_\n",
    "    best_scores_per_model[model.__class__.__name__] = rs.best_score_\n",
    "\n",
    "    # 결과 출력\n",
    "    print(f\"Best parameters for {model.__class__.__name__}: {rs.best_params_}\")\n",
    "    print(f\"Best cross-validation accuracy for {model.__class__.__name__}: {rs.best_score_:.4f}\")\n",
    "\n",
    "# 최적 모델로 훈련 후 검증 데이터 평가\n",
    "train_acc = net.score(X_train_np, y_train_np)\n",
    "val_acc = net.score(X_val_np, y_val_np)  # X_val_np와 y_val_np도 numpy로 변환 필요\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# 최종 결과 출력\n",
    "print(\"Best parameters for each model:\")\n",
    "print(best_params_per_model)\n",
    "print(\"Best cross-validation accuracy for each model:\")\n",
    "print(best_scores_per_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a46b87d-6a7d-47e6-b6c5-d097fb02baf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, train_loader, val_loader, num_epochs, learning_rate):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_accuracies, val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct / total\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        with torch.no_grad():\n",
    "            for features, labels in val_loader:\n",
    "                outputs = model(features)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "\n",
    "    # 마지막 평가 결과\n",
    "    accuracy = accuracy_score(val_correct, val_total)\n",
    "    f1 = f1_score(val_correct, val_total, average='weighted')\n",
    "\n",
    "    return accuracy, f1, train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86393598-fca7-4dc4-b941-c3d6b794e5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 곡선 시각화\n",
    "def plot_learning_curves(train_losses, val_losses, train_accuracies, val_accuracies):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # 손실 시각화\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label='Train Loss')\n",
    "    plt.plot(epochs, val_losses, label='Validation Loss')\n",
    "    plt.title('Loss Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # 정확도 시각화\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label='Train Accuracy')\n",
    "    plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
    "    plt.title('Accuracy Over Epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6505c08-57f9-4f3e-b511-f4338a7acc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 기반 앙상블 예측 함수\n",
    "def weighted_ensemble_predict(models, features, model_f1_scores):\n",
    "    model_predictions = []\n",
    "    weights = []\n",
    "\n",
    "    for i, model in enumerate(models):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            model_predictions.append(predicted.cpu().numpy())\n",
    "            weights.append(model_f1_scores[i])\n",
    "\n",
    "    model_predictions = np.array(model_predictions)\n",
    "    weights = np.array(weights)\n",
    "\n",
    "    weighted_predictions = np.zeros(model_predictions.shape[1], dtype=int)\n",
    "    for i in range(model_predictions.shape[1]):\n",
    "        weighted_sum = Counter()\n",
    "        for j in range(model_predictions.shape[0]):\n",
    "            weighted_sum[model_predictions[j, i]] += weights[j]\n",
    "        weighted_predictions[i] = weighted_sum.most_common(1)[0][0]\n",
    "\n",
    "    return weighted_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821e4730-a740-43d7-8c80-78f0885e5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "results = {}\n",
    "for model in models:\n",
    "    accuracy, f1, train_losses, val_losses, train_accuracies, val_accuracies = train_and_evaluate_model(\n",
    "        model, train_loader, val_loader, num_epochs=num_epochs, learning_rate=learning_rate\n",
    "    )\n",
    "    results[model.__class__.__name__] = {'accuracy': accuracy, 'f1': f1}\n",
    "    plot_learning_curves(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "\n",
    "best_model_name = max(results, key=lambda x: results[x]['f1'])\n",
    "best_model = [model for model in models if model.__class__.__name__ == best_model_name][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f95b3b9-d194-4c65-8af5-ae0d9fbb249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. 모델 저장 및 불러오기\n",
    "def save_model(model, filename):\n",
    "    torch.save(model.state_dict(), filename)\n",
    "\n",
    "def load_model(model, filename):\n",
    "    model.load_state_dict(torch.load(filename))\n",
    "    model.eval()\n",
    "\n",
    "save_model(best_model, './모델/best_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d6be8a-58a3-4e8b-91dd-fffe9e59cac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 자이로 데이터 예측 및 라벨링 추가\n",
    "gyro_tensor = torch.tensor(X_gyro_features, dtype=torch.float32)\n",
    "predicted_labels = weighted_ensemble_predict(models, gyro_tensor, model_f1_scores)\n",
    "gyro_sliced['predicted_label'] = predicted_labels\n",
    "gyro_sliced.to_csv('./원본 데이터/자이로 데이터_라벨링.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a884ec-baa7-49d3-9c74-5c7d5d0cabff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 예측 결과 시각화\n",
    "plt.figure(figsize=(10, 6))\n",
    "gyro_sliced['predicted_label'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Predicted Activity Distribution')\n",
    "plt.xlabel('Activity Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "gyro_sliced['predicted_label_numeric'] = gyro_sliced['predicted_label'].factorize()[0]\n",
    "plt.plot(gyro_sliced['RegisterDate'], gyro_sliced['predicted_label_numeric'], label='Predicted Label')\n",
    "plt.title('Activity Prediction Over Time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Predicted Activity (Numeric)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"자이로 데이터에 예측된 라벨을 추가한 결과 파일이 저장되었습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
