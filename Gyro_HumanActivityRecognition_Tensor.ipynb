{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "752b9046-01ee-4bb0-b506-c64951aa3f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6eeda1-2eae-40b1-af7d-b0a5035c9aed",
   "metadata": {},
   "source": [
    "# **1. 데이터 로드 및 전처리**\n",
    "***\n",
    "> 데이터를 로드하고 자이로스코프 데이터를 선택하고, 이를 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffe8848d-71a8-4a0f-8695-17b3294fa5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gyros_forearm_x    float64\n",
      "gyros_forearm_y    float64\n",
      "gyros_forearm_z    float64\n",
      "dtype: object\n",
      "gyros_forearm_x    0\n",
      "gyros_forearm_y    0\n",
      "gyros_forearm_z    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 데이터 읽기 및 전처리\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    CSV 파일에서 데이터를 로드하고 자이로스코프 데이터를 선택하여 정규화한 후, \n",
    "    추가적인 특징을 생성하는 함수\n",
    "    \"\"\"\n",
    "\n",
    "    # 숫자형 데이터만 로드하기 위해 dtype을 지정\n",
    "    data = pd.read_csv(file_path, dtype={\n",
    "        'gyros_forearm_x': float,\n",
    "        'gyros_forearm_y': float,\n",
    "        'gyros_forearm_z': float\n",
    "    }, low_memory=False)\n",
    "\n",
    "    # 사용할 특징: 자이로스코프 데이터\n",
    "    features = data[['gyros_forearm_x', 'gyros_forearm_y', 'gyros_forearm_z']]\n",
    "    labels = data['classe']\n",
    "\n",
    "    # 데이터가 숫자형으로 변환되지 않은 비정상적인 값을 포함하고 있는지 확인\n",
    "    print(features.dtypes)\n",
    "    print(features.isnull().sum())\n",
    "\n",
    "    # 데이터 정규화: 각 특징을 평균이 0, 표준편차가 1이 되도록 변환\n",
    "    features = (features - features.mean()) / features.std()\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "# pml-training 데이터 로드 및 전처리\n",
    "training_data_path = './원본 데이터/pml-training.csv'\n",
    "features, labels = load_and_preprocess_data(training_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c526de7-e564-470c-b9cb-2394d384eb4b",
   "metadata": {},
   "source": [
    "# **2. 특징 공학 (Feature Engineering)**\n",
    "***\n",
    "> 추가적인 특징을 생성하여 모델에 더 유의미한 입력을 제공\n",
    "> 각 축에 대한 통계적 특징(평균, 표준편차, 최댓값, 최솟값, 범위)과 자이로스코프 데이터의 벡터 크기를 계산하여 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5a8722e-de23-4c2f-9509-cf3e28c215c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 추가적인 특징 생성 함수\n",
    "def feature_engineering(df):\n",
    "    \"\"\"\n",
    "    자이로스코프 데이터에서 추가적인 특징을 생성하는 함수\n",
    "    \"\"\"\n",
    "\n",
    "    # 각 축에 대해 통계량 계산\n",
    "    for axis in ['x', 'y', 'z']:\n",
    "        df[f'gyros_forearm_{axis}_mean'] = df[f'gyros_forearm_{axis}'].mean()\n",
    "        df[f'gyros_forearm_{axis}_std'] = df[f'gyros_forearm_{axis}'].std()\n",
    "        df[f'gyros_forearm_{axis}_max'] = df[f'gyros_forearm_{axis}'].max()\n",
    "        df[f'gyros_forearm_{axis}_min'] = df[f'gyros_forearm_{axis}'].min()\n",
    "        df[f'gyros_forearm_{axis}_range'] = df[f'gyros_forearm_{axis}_max'] - df[f'gyros_forearm_{axis}_min']\n",
    "\n",
    "    # 벡터 크기 계산\n",
    "    df['gyros_forearm_magnitude'] = np.sqrt(df['gyros_forearm_x'] ** 2 + df['gyros_forearm_y'] ** 2 + df['gyros_forearm_z'] ** 2)\n",
    "\n",
    "    return df\n",
    "\n",
    "# 추가적인 특징 생성\n",
    "features = feature_engineering(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b91e5-e647-48d8-9ac2-0a11910d2679",
   "metadata": {},
   "source": [
    "# **3. 데이터 분할 및 라벨 인코딩**\n",
    "***\n",
    "> 데이터를 학습용과 테스트용으로 분할하고, 라벨을 신경망에서 사용할 수 있도록 원-핫 인코딩을 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "833a8e8b-d699-4595-975f-bd8be844c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분할 및 라벨 인코딩\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)\n",
    "\n",
    "# 입력 데이터 차원 확장\n",
    "x_train = np.expand_dims(x_train, axis=1) # (샘플 수, 시퀸스 길이, 1)로 변환\n",
    "x_test = np.expand_dims(x_test, axis=1) # (샘플 수, 시퀸스 길이, 1)로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eeca5e-14dd-4792-b52b-4dd6eac8029c",
   "metadata": {},
   "source": [
    "# **4. 모델 생성 및 학습**\n",
    "***\n",
    "> 이 단계에서는 CNN+LSTM 구조의 모델을 생성하고, 학습을 진행, 모델의 성능을 최적화하기 위해 조기 종료(Early Stopping)와 모델 체크포인트 저장(Model Checkpoint)을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "204e5971-2469-4ff7-83d9-8dee2c96ee5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_2 (Conv1D)           (None, 1, 64)             2496      \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 1, 64)            256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 1, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 1, 64)             8256      \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 1, 64)            256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling1d_3 (MaxPooling  (None, 1, 64)            0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 1, 64)             33024     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1, 64)             0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 64)                33024     \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               8320      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 86,277\n",
      "Trainable params: 86,021\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "491/491 - 10s - loss: 1.5848 - accuracy: 0.2852 - val_loss: 1.5918 - val_accuracy: 0.2902 - 10s/epoch - 20ms/step\n",
      "Epoch 2/100\n",
      "491/491 - 3s - loss: 1.5541 - accuracy: 0.3153 - val_loss: 1.5764 - val_accuracy: 0.2836 - 3s/epoch - 5ms/step\n",
      "Epoch 3/100\n",
      "491/491 - 2s - loss: 1.5376 - accuracy: 0.3201 - val_loss: 1.5991 - val_accuracy: 0.2899 - 2s/epoch - 5ms/step\n",
      "Epoch 4/100\n",
      "491/491 - 2s - loss: 1.5301 - accuracy: 0.3217 - val_loss: 1.5787 - val_accuracy: 0.2831 - 2s/epoch - 5ms/step\n",
      "Epoch 5/100\n",
      "491/491 - 2s - loss: 1.5243 - accuracy: 0.3292 - val_loss: 1.5897 - val_accuracy: 0.2843 - 2s/epoch - 5ms/step\n",
      "Epoch 6/100\n",
      "491/491 - 2s - loss: 1.5199 - accuracy: 0.3287 - val_loss: 1.8259 - val_accuracy: 0.1952 - 2s/epoch - 5ms/step\n",
      "Epoch 7/100\n",
      "491/491 - 2s - loss: 1.5151 - accuracy: 0.3323 - val_loss: 1.5744 - val_accuracy: 0.2932 - 2s/epoch - 5ms/step\n",
      "Epoch 8/100\n",
      "491/491 - 2s - loss: 1.5171 - accuracy: 0.3336 - val_loss: 1.6069 - val_accuracy: 0.2833 - 2s/epoch - 5ms/step\n",
      "Epoch 9/100\n",
      "491/491 - 2s - loss: 1.5127 - accuracy: 0.3339 - val_loss: 1.6392 - val_accuracy: 0.2596 - 2s/epoch - 5ms/step\n",
      "Epoch 10/100\n",
      "491/491 - 2s - loss: 1.5125 - accuracy: 0.3375 - val_loss: 1.5962 - val_accuracy: 0.2864 - 2s/epoch - 5ms/step\n",
      "Epoch 11/100\n",
      "491/491 - 2s - loss: 1.5102 - accuracy: 0.3392 - val_loss: 1.5439 - val_accuracy: 0.3167 - 2s/epoch - 4ms/step\n",
      "Epoch 12/100\n",
      "491/491 - 2s - loss: 1.5099 - accuracy: 0.3408 - val_loss: 1.6046 - val_accuracy: 0.2861 - 2s/epoch - 5ms/step\n",
      "Epoch 13/100\n",
      "491/491 - 3s - loss: 1.5056 - accuracy: 0.3431 - val_loss: 1.7392 - val_accuracy: 0.1957 - 3s/epoch - 6ms/step\n",
      "Epoch 14/100\n",
      "491/491 - 2s - loss: 1.5018 - accuracy: 0.3463 - val_loss: 1.5923 - val_accuracy: 0.2864 - 2s/epoch - 4ms/step\n",
      "Epoch 15/100\n",
      "491/491 - 2s - loss: 1.5021 - accuracy: 0.3459 - val_loss: 1.6213 - val_accuracy: 0.2688 - 2s/epoch - 4ms/step\n",
      "Epoch 16/100\n",
      "491/491 - 2s - loss: 1.5010 - accuracy: 0.3453 - val_loss: 1.6097 - val_accuracy: 0.2013 - 2s/epoch - 4ms/step\n",
      "Epoch 17/100\n",
      "491/491 - 3s - loss: 1.4962 - accuracy: 0.3494 - val_loss: 1.7456 - val_accuracy: 0.2910 - 3s/epoch - 5ms/step\n",
      "Epoch 18/100\n",
      "491/491 - 4s - loss: 1.4937 - accuracy: 0.3524 - val_loss: 1.5968 - val_accuracy: 0.1875 - 4s/epoch - 7ms/step\n",
      "Epoch 19/100\n",
      "491/491 - 3s - loss: 1.4968 - accuracy: 0.3488 - val_loss: 1.6915 - val_accuracy: 0.2828 - 3s/epoch - 6ms/step\n",
      "Epoch 20/100\n",
      "491/491 - 3s - loss: 1.4983 - accuracy: 0.3514 - val_loss: 1.6803 - val_accuracy: 0.2882 - 3s/epoch - 6ms/step\n",
      "Epoch 21/100\n",
      "491/491 - 3s - loss: 1.4980 - accuracy: 0.3501 - val_loss: 2.1188 - val_accuracy: 0.1771 - 3s/epoch - 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# CNN+LSTM 모델 생성 함수\n",
    "def create_cnn_model(input_shape, num_classes):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=2, strides=1, padding='same', activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool1D(pool_size=1),\n",
    "        \n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=2, strides=1, padding='same', activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPool1D(pool_size=1),\n",
    "\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        \n",
    "        tf.keras.layers.Dense(units=128, activation='relu'),\n",
    "        tf.keras.layers.Dropout(rate=0.5),\n",
    "        tf.keras.layers.Dense(units=num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 모델 생성\n",
    "input_shape = (x_train.shape[1], x_train.shape[2])\n",
    "num_classes = y_train.shape[1]\n",
    "model = create_cnn_model(input_shape, num_classes)\n",
    "model.summary()\n",
    "\n",
    "# 콜백 함수 정의\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss')\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90e938-5794-4e96-a5d9-9da11b6d1f6a",
   "metadata": {},
   "source": [
    "# **5. 모델 평가**\n",
    "***\n",
    "> 학습된 모델을 평가하여 훈련데이터와 테스트 데이터에서의 성능을 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3461ef2-1243-42c8-ba0a-93833d4f28fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491/491 - 1s - loss: 1.5374 - accuracy: 0.3220 - 824ms/epoch - 2ms/step\n",
      "123/123 - 0s - loss: 1.5439 - accuracy: 0.3167 - 212ms/epoch - 2ms/step\n",
      "훈련 정확도: 0.3219723403453827, 테스트 정확도: 0.3166879117488861\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터와 테스트 데이터에 대한 성능 평가\n",
    "train_loss, train_acc = model.evaluate(x_train, y_train, verbose=2)\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "print(f\"훈련 정확도: {train_acc}, 테스트 정확도: {test_acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55de5459-0f1b-4707-831a-bb572b7f3904",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
